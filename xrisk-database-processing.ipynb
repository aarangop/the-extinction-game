{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Existential Risk Estimates Database\n",
    "\n",
    "This notebook processes data from the existential risk (x-risk) estimates database from [this EA Forum post](https://forum.effectivealtruism.org/posts/JQQAQrunyGGhzE23a/database-of-existential-risk-estimates).\n",
    "\n",
    "Since the entries in the database vary in format and often use natural language, it is necessary to sanitize them for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "In this step we fiddle with the dataset to get something more readily usable. Among other things, we:\n",
    "- Create a risk category column, and get rid of the \"headers\" for things like AI, or Total Risk, etc.\n",
    "- Remove columns that aren't necessary for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Overall notes',\n",
       " 'Existential-risk-level estimate',\n",
       " 'Conditional existential-risk-le',\n",
       " 'Estimates of somewhat less extr',\n",
       " 'Other potential estimates or so']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data and extract the relevant columns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load excel spreadsheet. Load all sheets.\n",
    "excel = pd.ExcelFile('./data/xrisk-estimates-database-20241204.xlsx')\n",
    "# List worksheets\n",
    "excel.sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Who is the estimator?</th>\n",
       "      <th>When was the estimate made/published?</th>\n",
       "      <th>What is the estimator estimating?</th>\n",
       "      <th>What is their estimate?</th>\n",
       "      <th>Source</th>\n",
       "      <th>Have I properly read the source myself?</th>\n",
       "      <th>Is this estimate included in Beard et al.'s appendix?</th>\n",
       "      <th>Other notes</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Total risk” (or similar)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toby Ord</td>\n",
       "      <td>2020</td>\n",
       "      <td>“Total existential risk” by 2120</td>\n",
       "      <td>~17% (~1 in 6)</td>\n",
       "      <td>The Precipice</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Ord writes: \"Don’t take these numbers to be co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GCR Conference</td>\n",
       "      <td>2008</td>\n",
       "      <td>“Overall risk of extinction prior to 2100”</td>\n",
       "      <td>0.19</td>\n",
       "      <td>https://www.fhi.ox.ac.uk/reports/2008-1.pdf</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>This is the median. The report about these est...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Will MacAskill</td>\n",
       "      <td>2019/2020</td>\n",
       "      <td>Existential risk in the 21st century</td>\n",
       "      <td>0.01</td>\n",
       "      <td>https://80000hours.org/podcast/episodes/will-m...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben Todd or 80,000 Hours</td>\n",
       "      <td>2017</td>\n",
       "      <td>Extinction risk “in the next century”</td>\n",
       "      <td>Probably at or above 3%</td>\n",
       "      <td>https://80000hours.org/articles/extinction-risk/</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Who is the estimator?  When was the estimate made/published?  \\\n",
       "0  “Total risk” (or similar)                                   NaN   \n",
       "1                   Toby Ord                                  2020   \n",
       "2             GCR Conference                                  2008   \n",
       "3             Will MacAskill                             2019/2020   \n",
       "4   Ben Todd or 80,000 Hours                                  2017   \n",
       "\n",
       "            What is the estimator estimating?  What is their estimate?  \\\n",
       "0                                         NaN                      NaN   \n",
       "1            “Total existential risk” by 2120           ~17% (~1 in 6)   \n",
       "2  “Overall risk of extinction prior to 2100”                     0.19   \n",
       "3        Existential risk in the 21st century                     0.01   \n",
       "4       Extinction risk “in the next century”  Probably at or above 3%   \n",
       "\n",
       "                                              Source  \\\n",
       "0                                                NaN   \n",
       "1                                      The Precipice   \n",
       "2        https://www.fhi.ox.ac.uk/reports/2008-1.pdf   \n",
       "3  https://80000hours.org/podcast/episodes/will-m...   \n",
       "4   https://80000hours.org/articles/extinction-risk/   \n",
       "\n",
       "  Have I properly read the source myself?  \\\n",
       "0                                     NaN   \n",
       "1                                     Yes   \n",
       "2                                     Yes   \n",
       "3                                     Yes   \n",
       "4                                     Yes   \n",
       "\n",
       "  Is this estimate included in Beard et al.'s appendix?  \\\n",
       "0                                                NaN      \n",
       "1                                                 No      \n",
       "2                                                Yes      \n",
       "3                                                 No      \n",
       "4                                                 No      \n",
       "\n",
       "                                         Other notes  Unnamed: 8 Unnamed: 9  \n",
       "0                                                NaN         NaN        NaN  \n",
       "1  Ord writes: \"Don’t take these numbers to be co...         NaN        NaN  \n",
       "2  This is the median. The report about these est...         NaN        NaN  \n",
       "3                                                NaN         NaN        NaN  \n",
       "4                                                NaN         NaN        NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the 'Existential-risk-level estimate' worksheet.\n",
    "# the first 5 columns are instructions, so skip them.\n",
    "df = excel.parse('Existential-risk-level estimate', skiprows=5, index_row=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has 10 columns\n"
     ]
    }
   ],
   "source": [
    "# Print number of columns\n",
    "print(f\"DataFrame has {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Who is the estimator? ', 'When was the estimate made/published?',\n",
       "       'What is the estimator estimating?', 'What is their estimate?',\n",
       "       'Source', 'Have I properly read the source myself?',\n",
       "       'Is this estimate included in Beard et al.'s appendix?', 'Other notes',\n",
       "       'Unnamed: 8', 'Unnamed: 9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>date</th>\n",
       "      <th>estimation_measure</th>\n",
       "      <th>estimation</th>\n",
       "      <th>source</th>\n",
       "      <th>source_read_by_estimator</th>\n",
       "      <th>estimate_included_in_beard_et_al</th>\n",
       "      <th>other_notes</th>\n",
       "      <th>unknown_column_1</th>\n",
       "      <th>unknown_column_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Total risk” (or similar)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toby Ord</td>\n",
       "      <td>2020</td>\n",
       "      <td>“Total existential risk” by 2120</td>\n",
       "      <td>~17% (~1 in 6)</td>\n",
       "      <td>The Precipice</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Ord writes: \"Don’t take these numbers to be co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GCR Conference</td>\n",
       "      <td>2008</td>\n",
       "      <td>“Overall risk of extinction prior to 2100”</td>\n",
       "      <td>0.19</td>\n",
       "      <td>https://www.fhi.ox.ac.uk/reports/2008-1.pdf</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>This is the median. The report about these est...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Will MacAskill</td>\n",
       "      <td>2019/2020</td>\n",
       "      <td>Existential risk in the 21st century</td>\n",
       "      <td>0.01</td>\n",
       "      <td>https://80000hours.org/podcast/episodes/will-m...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben Todd or 80,000 Hours</td>\n",
       "      <td>2017</td>\n",
       "      <td>Extinction risk “in the next century”</td>\n",
       "      <td>Probably at or above 3%</td>\n",
       "      <td>https://80000hours.org/articles/extinction-risk/</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   estimator       date  \\\n",
       "0  “Total risk” (or similar)        NaN   \n",
       "1                   Toby Ord       2020   \n",
       "2             GCR Conference       2008   \n",
       "3             Will MacAskill  2019/2020   \n",
       "4   Ben Todd or 80,000 Hours       2017   \n",
       "\n",
       "                           estimation_measure               estimation  \\\n",
       "0                                         NaN                      NaN   \n",
       "1            “Total existential risk” by 2120           ~17% (~1 in 6)   \n",
       "2  “Overall risk of extinction prior to 2100”                     0.19   \n",
       "3        Existential risk in the 21st century                     0.01   \n",
       "4       Extinction risk “in the next century”  Probably at or above 3%   \n",
       "\n",
       "                                              source source_read_by_estimator  \\\n",
       "0                                                NaN                      NaN   \n",
       "1                                      The Precipice                      Yes   \n",
       "2        https://www.fhi.ox.ac.uk/reports/2008-1.pdf                      Yes   \n",
       "3  https://80000hours.org/podcast/episodes/will-m...                      Yes   \n",
       "4   https://80000hours.org/articles/extinction-risk/                      Yes   \n",
       "\n",
       "  estimate_included_in_beard_et_al  \\\n",
       "0                              NaN   \n",
       "1                               No   \n",
       "2                              Yes   \n",
       "3                               No   \n",
       "4                               No   \n",
       "\n",
       "                                         other_notes  unknown_column_1  \\\n",
       "0                                                NaN               NaN   \n",
       "1  Ord writes: \"Don’t take these numbers to be co...               NaN   \n",
       "2  This is the median. The report about these est...               NaN   \n",
       "3                                                NaN               NaN   \n",
       "4                                                NaN               NaN   \n",
       "\n",
       "  unknown_column_2  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns to something more concise\n",
    "new_column_names = [\n",
    "  'estimator',\n",
    "  'date',\n",
    "  'estimation_measure',\n",
    "  'estimation',\n",
    "  'source',\n",
    "  'source_read_by_estimator',\n",
    "  'estimate_included_in_beard_et_al',\n",
    "  'other_notes',\n",
    "  'unknown_column_1',\n",
    "'unknown_column_2'\n",
    "]\n",
    "df.columns = new_column_names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>date</th>\n",
       "      <th>estimation_measure</th>\n",
       "      <th>estimation</th>\n",
       "      <th>source</th>\n",
       "      <th>source_read_by_estimator</th>\n",
       "      <th>estimate_included_in_beard_et_al</th>\n",
       "      <th>other_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>GCR Conference</td>\n",
       "      <td>2008</td>\n",
       "      <td>Human extinction by 2100 as a result of “wars ...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>https://www.fhi.ox.ac.uk/reports/2008-1.pdf</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>This is the median. Beard et al.'s appendix sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Toby Ord</td>\n",
       "      <td>2020</td>\n",
       "      <td>“Total natural [existential] risk” by 2120</td>\n",
       "      <td>~0.01% (~1 in 10,000)</td>\n",
       "      <td>The Precipice</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Paul Christiano</td>\n",
       "      <td>2021</td>\n",
       "      <td>Expected fraction of total potential value tha...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Comment</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Response to Wei Dai's question shown in the ab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           estimator  date                                 estimation_measure  \\\n",
       "90    GCR Conference  2008  Human extinction by 2100 as a result of “wars ...   \n",
       "84          Toby Ord  2020         “Total natural [existential] risk” by 2120   \n",
       "100  Paul Christiano  2021  Expected fraction of total potential value tha...   \n",
       "\n",
       "                estimation                                       source  \\\n",
       "90                    0.04  https://www.fhi.ox.ac.uk/reports/2008-1.pdf   \n",
       "84   ~0.01% (~1 in 10,000)                                The Precipice   \n",
       "100                    0.2                                      Comment   \n",
       "\n",
       "    source_read_by_estimator estimate_included_in_beard_et_al  \\\n",
       "90                       Yes                              Yes   \n",
       "84                       Yes                               No   \n",
       "100                      Yes                               No   \n",
       "\n",
       "                                           other_notes  \n",
       "90   This is the median. Beard et al.'s appendix sa...  \n",
       "84                                                 NaN  \n",
       "100  Response to Wei Dai's question shown in the ab...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unnecessary columns (last 2, which are unknown)\n",
    "df = df.drop(columns=['unknown_column_1', 'unknown_column_2'])\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>date</th>\n",
       "      <th>estimation_measure</th>\n",
       "      <th>estimation</th>\n",
       "      <th>source</th>\n",
       "      <th>source_read_by_estimator</th>\n",
       "      <th>estimate_included_in_beard_et_al</th>\n",
       "      <th>other_notes</th>\n",
       "      <th>risk_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Nanotechnology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Toby Ord</td>\n",
       "      <td>2020</td>\n",
       "      <td>Existential catastrophe by 2120 as a result of...</td>\n",
       "      <td>~10%</td>\n",
       "      <td>The Precipice</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Commentary here: https://forum.effectivealtrui...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         estimator  date                                 estimation_measure  \\\n",
       "94             NaN   NaN                                                NaN   \n",
       "51  Nanotechnology   NaN                                                NaN   \n",
       "25        Toby Ord  2020  Existential catastrophe by 2120 as a result of...   \n",
       "\n",
       "   estimation         source source_read_by_estimator  \\\n",
       "94        NaN            NaN                      NaN   \n",
       "51        NaN            NaN                      NaN   \n",
       "25       ~10%  The Precipice                      Yes   \n",
       "\n",
       "   estimate_included_in_beard_et_al  \\\n",
       "94                              NaN   \n",
       "51                              NaN   \n",
       "25                               No   \n",
       "\n",
       "                                          other_notes  risk_category  \n",
       "94                                                NaN            NaN  \n",
       "51                                                NaN            NaN  \n",
       "25  Commentary here: https://forum.effectivealtrui...            NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add empty column for risk category\n",
    "df['risk_category'] = np.nan\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             “Total risk” (or similar)\n",
       "17                                                   AI\n",
       "39                                              Biorisk\n",
       "51                                       Nanotechnology\n",
       "78          Natural risks (excluding natural pandemics)\n",
       "87                                                  War\n",
       "92    Explicitly about only unrecoverable dystopia a...\n",
       "95                                        Miscellaneous\n",
       "Name: estimator, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first row indicates what type of risk is being estimated.\n",
    "# \n",
    "# Under the first row are the total risk estimates. \n",
    "# \n",
    "# Further down below are estimates for other x-risks.\n",
    "# \n",
    "# The categories are: \"Total risk (or similar)\", \"AI\", \"Biorisk\", \"Nanotechnology\", \"Climate Change\", \"Natural risks (excluding natural pandemics)\", \"War\", \"Explicitly about only unrecoverable dystopia and/or unrecoverable collapse (not extinction)\", \"Miscellaneous\".\n",
    "risk_categories = [\n",
    "    '“Total risk” (or similar)',\n",
    "    'AI',\n",
    "    'Biorisk',\n",
    "    'Nanotechnology',\n",
    "    'Climate Change',\n",
    "    'Natural risks (excluding natural pandemics)',\n",
    "    'War',\n",
    "    'Explicitly about only unrecoverable dystopia and/or unrecoverable collapse (not extinction)',\n",
    "    'Miscellaneous']\n",
    "\n",
    "risk_categories_aliases = [\n",
    "    'total',\n",
    "    'ai',\n",
    "    'biorisk',\n",
    "    'nanotechnology',\n",
    "    'climate_change',\n",
    "    'natural_risks',\n",
    "    'war',\n",
    "    'dystopia',\n",
    "    'miscellaneous'\n",
    "]\n",
    "\n",
    "# Get rows that contain a risk category in the first column\n",
    "risk_category_rows = df[df['estimator'].isin(risk_categories)]['estimator']\n",
    "risk_category_rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/134vcy4x6c19634zvbkbff900000gn/T/ipykernel_37345/2712489716.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '“Total risk” (or similar)' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_with_risk_category.at[i, 'risk_category'] = current_risk_category\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>date</th>\n",
       "      <th>estimation_measure</th>\n",
       "      <th>estimation</th>\n",
       "      <th>source</th>\n",
       "      <th>source_read_by_estimator</th>\n",
       "      <th>estimate_included_in_beard_et_al</th>\n",
       "      <th>other_notes</th>\n",
       "      <th>risk_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Will MacAskill</td>\n",
       "      <td>2019/2020</td>\n",
       "      <td>Existential risk in the 21st century</td>\n",
       "      <td>0.01</td>\n",
       "      <td>https://80000hours.org/podcast/episodes/will-m...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>“Total risk” (or similar)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Brian Tomasik</td>\n",
       "      <td>2015/2018</td>\n",
       "      <td>Chance that \"Humans will go extinct within mil...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>https://reducing-suffering.org/summary-beliefs...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Discussed here: https://www.facebook.com/brian...</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Pablo Stafforini</td>\n",
       "      <td>2015/2020</td>\n",
       "      <td>Chance that \"Humans will go extinct within mil...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>http://www.stafforini.com/blog/what_i_believe/</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            estimator       date  \\\n",
       "3      Will MacAskill  2019/2020   \n",
       "109     Brian Tomasik  2015/2018   \n",
       "110  Pablo Stafforini  2015/2020   \n",
       "\n",
       "                                    estimation_measure estimation  \\\n",
       "3                 Existential risk in the 21st century       0.01   \n",
       "109  Chance that \"Humans will go extinct within mil...       0.05   \n",
       "110  Chance that \"Humans will go extinct within mil...        0.1   \n",
       "\n",
       "                                                source  \\\n",
       "3    https://80000hours.org/podcast/episodes/will-m...   \n",
       "109  https://reducing-suffering.org/summary-beliefs...   \n",
       "110     http://www.stafforini.com/blog/what_i_believe/   \n",
       "\n",
       "    source_read_by_estimator estimate_included_in_beard_et_al  \\\n",
       "3                        Yes                               No   \n",
       "109                      Yes                               No   \n",
       "110                      Yes                               No   \n",
       "\n",
       "                                           other_notes  \\\n",
       "3                                                  NaN   \n",
       "109  Discussed here: https://www.facebook.com/brian...   \n",
       "110                                                NaN   \n",
       "\n",
       "                 risk_category  \n",
       "3    “Total risk” (or similar)  \n",
       "109              Miscellaneous  \n",
       "110              Miscellaneous  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a copy of the dataframe\n",
    "df_with_risk_category = df.copy()\n",
    "\n",
    "# Initialize the current risk category\n",
    "current_risk_category = None\n",
    "\n",
    "# Iterate over the dataframe rows\n",
    "for i, row in df_with_risk_category.iterrows():\n",
    "  if row['estimator'] in risk_category_rows.values:\n",
    "    # Update the current risk category\n",
    "    current_risk_category = row['estimator']\n",
    "  # Set the risk category for the current row\n",
    "  df_with_risk_category.at[i, 'risk_category'] = current_risk_category\n",
    "\n",
    "df_with_risk_category.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>date</th>\n",
       "      <th>estimation_measure</th>\n",
       "      <th>estimation</th>\n",
       "      <th>source</th>\n",
       "      <th>source_read_by_estimator</th>\n",
       "      <th>estimate_included_in_beard_et_al</th>\n",
       "      <th>other_notes</th>\n",
       "      <th>risk_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toby Ord</td>\n",
       "      <td>2020</td>\n",
       "      <td>“Total existential risk” by 2120</td>\n",
       "      <td>~17% (~1 in 6)</td>\n",
       "      <td>The Precipice</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Ord writes: \"Don’t take these numbers to be co...</td>\n",
       "      <td>“Total risk” (or similar)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GCR Conference</td>\n",
       "      <td>2008</td>\n",
       "      <td>“Overall risk of extinction prior to 2100”</td>\n",
       "      <td>0.19</td>\n",
       "      <td>https://www.fhi.ox.ac.uk/reports/2008-1.pdf</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>This is the median. The report about these est...</td>\n",
       "      <td>“Total risk” (or similar)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Will MacAskill</td>\n",
       "      <td>2019/2020</td>\n",
       "      <td>Existential risk in the 21st century</td>\n",
       "      <td>0.01</td>\n",
       "      <td>https://80000hours.org/podcast/episodes/will-m...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>“Total risk” (or similar)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        estimator       date                          estimation_measure  \\\n",
       "1        Toby Ord       2020            “Total existential risk” by 2120   \n",
       "2  GCR Conference       2008  “Overall risk of extinction prior to 2100”   \n",
       "3  Will MacAskill  2019/2020        Existential risk in the 21st century   \n",
       "\n",
       "       estimation                                             source  \\\n",
       "1  ~17% (~1 in 6)                                      The Precipice   \n",
       "2            0.19        https://www.fhi.ox.ac.uk/reports/2008-1.pdf   \n",
       "3            0.01  https://80000hours.org/podcast/episodes/will-m...   \n",
       "\n",
       "  source_read_by_estimator estimate_included_in_beard_et_al  \\\n",
       "1                      Yes                               No   \n",
       "2                      Yes                              Yes   \n",
       "3                      Yes                               No   \n",
       "\n",
       "                                         other_notes  \\\n",
       "1  Ord writes: \"Don’t take these numbers to be co...   \n",
       "2  This is the median. The report about these est...   \n",
       "3                                                NaN   \n",
       "\n",
       "               risk_category  \n",
       "1  “Total risk” (or similar)  \n",
       "2  “Total risk” (or similar)  \n",
       "3  “Total risk” (or similar)  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the rows that contain the risk categories\n",
    "df_with_risk_category = df_with_risk_category[~df_with_risk_category['estimator'].isin(risk_categories)]\n",
    "df_with_risk_category.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>date</th>\n",
       "      <th>estimation_measure</th>\n",
       "      <th>estimation</th>\n",
       "      <th>source</th>\n",
       "      <th>source_read_by_estimator</th>\n",
       "      <th>estimate_included_in_beard_et_al</th>\n",
       "      <th>other_notes</th>\n",
       "      <th>risk_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Wei Dai</td>\n",
       "      <td>2021</td>\n",
       "      <td>Expected fraction of total potential value tha...</td>\n",
       "      <td>&gt;50%</td>\n",
       "      <td>Comment</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>\"What's your expectation of the fraction of to...</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Climate change</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nanotechnology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Toby Ord</td>\n",
       "      <td>2020</td>\n",
       "      <td>Existential catastrophe from “other anthropoge...</td>\n",
       "      <td>~2% (~1 in 50)</td>\n",
       "      <td>The Precipice</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>See this post for some commentary: https://for...</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          estimator  date                                 estimation_measure  \\\n",
       "99          Wei Dai  2021  Expected fraction of total potential value tha...   \n",
       "72   Climate change   NaN                                                NaN   \n",
       "104        Toby Ord  2020  Existential catastrophe from “other anthropoge...   \n",
       "\n",
       "         estimation         source source_read_by_estimator  \\\n",
       "99             >50%        Comment                      Yes   \n",
       "72              NaN            NaN                      NaN   \n",
       "104  ~2% (~1 in 50)  The Precipice                      Yes   \n",
       "\n",
       "    estimate_included_in_beard_et_al  \\\n",
       "99                                No   \n",
       "72                               NaN   \n",
       "104                               No   \n",
       "\n",
       "                                           other_notes   risk_category  \n",
       "99   \"What's your expectation of the fraction of to...   miscellaneous  \n",
       "72                                                 NaN  nanotechnology  \n",
       "104  See this post for some commentary: https://for...   miscellaneous  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename risk categories to something more concise\n",
    "df_with_risk_category['risk_category'] = df_with_risk_category['risk_category'].replace(risk_categories, risk_categories_aliases)\n",
    "df_with_risk_category.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>estimation_measure</th>\n",
       "      <th>date</th>\n",
       "      <th>estimation</th>\n",
       "      <th>source_read_by_estimator</th>\n",
       "      <th>risk_category</th>\n",
       "      <th>other_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toby Ord</td>\n",
       "      <td>“Total existential risk” by 2120</td>\n",
       "      <td>2020</td>\n",
       "      <td>~17% (~1 in 6)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>total</td>\n",
       "      <td>Ord writes: \"Don’t take these numbers to be co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GCR Conference</td>\n",
       "      <td>“Overall risk of extinction prior to 2100”</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.19</td>\n",
       "      <td>Yes</td>\n",
       "      <td>total</td>\n",
       "      <td>This is the median. The report about these est...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Will MacAskill</td>\n",
       "      <td>Existential risk in the 21st century</td>\n",
       "      <td>2019/2020</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Yes</td>\n",
       "      <td>total</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        estimator                          estimation_measure       date  \\\n",
       "1        Toby Ord            “Total existential risk” by 2120       2020   \n",
       "2  GCR Conference  “Overall risk of extinction prior to 2100”       2008   \n",
       "3  Will MacAskill        Existential risk in the 21st century  2019/2020   \n",
       "\n",
       "       estimation source_read_by_estimator risk_category  \\\n",
       "1  ~17% (~1 in 6)                      Yes         total   \n",
       "2            0.19                      Yes         total   \n",
       "3            0.01                      Yes         total   \n",
       "\n",
       "                                         other_notes  \n",
       "1  Ord writes: \"Don’t take these numbers to be co...  \n",
       "2  This is the median. The report about these est...  \n",
       "3                                                NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use only a few columns relevant for the analysis.\n",
    "relevant_columns = [\n",
    "    \"estimator\",\n",
    "    \"estimation_measure\",\n",
    "    \"date\",\n",
    "    \"estimation\",\n",
    "    \"source_read_by_estimator\",\n",
    "    \"risk_category\",\n",
    "    \"other_notes\"\n",
    "]\n",
    "df_relevant = df_with_risk_category[relevant_columns]\n",
    "df_relevant.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a CSV file\n",
    "df_relevant.to_csv('./data/pre-processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking out the big guns\n",
    "\n",
    "This might be total overkill for our purposes, but I find it fun, so, we'll use an LLM to help us process the rest of the columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Estimation measure and estimation\n",
    "\n",
    "The \"estimation measure\" varies slightly from estimation to estimation. It will be necessary to harmonize this into something more usable, like current existential risk per century.\n",
    "\n",
    "For this, I'll attempt using an LLM to help us with this \"translation\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_load_model_from_file: using device Metal (Apple M1 Pro) - 10916 MiB free\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./models/llama-2-7b.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x11c852a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x11c8593a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x14efecda0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x10f2a0280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x10f2a1a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x119441f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x10f5acac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x1194435c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x11c859b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x14eff62e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x11c85a200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x14eff6510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x11c66c700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x11c639630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x14eff6ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x11c85afa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x10f40a810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x11c639860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x11c639d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x11c63a460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x14eff7640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x14eff7b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x14eff8770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x11c85b450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x11c63a690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11c63adc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x11c63b190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11c63b580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x11c63b7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11c63bba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x11c85b840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x14eff8e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11c85c140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11c63bdd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11c6354d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11c638630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f5adca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11c85cb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11c6358c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11c85cdb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1194454b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11c85d340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11c635af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11c636300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11c636ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11c85d9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14eff9730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14eff9960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11c637110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f774790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11c6378d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x11c66ef60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x11c66f750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x11c85dc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x11c85de80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11c85edc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14effa0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14effa320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11c66ff10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11c85f390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14effa710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11c6706f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11c670920 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f5ad680 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11c670b50 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11c85f960 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11c670d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11c860b50 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x119442420 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11c670fb0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11c6711e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11c8602f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11c861300 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11c860520 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14effab00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11d6b15b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11d6b17e0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f7712a0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14effadb0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14effafe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11c8626b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11c862aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11c863c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f40ac60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11d6b1a10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14effbfc0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11c8631b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11d6b2550 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14effc440 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14effd3d0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11d6b3600 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11d6b2870 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11d6b4000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11d6b4860 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11d6b4f30 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f770040 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11c8633e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11c864d40 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11c865750 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f40bd20 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f40bf50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11d6b5a10 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11c866570 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11c865a50 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14effc960 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11c865d00 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14effcb90 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11d6b6570 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11d6b68c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1194468f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11d6b6cb0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11c8677d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11d6b6ee0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14effdd70 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11d6b75b0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11d6b7cf0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14effed20 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11d6b8610 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11c868720 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11d6b9080 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14eff5690 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11d6b92b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11d6ba2d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14efff6f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11c868cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11d6bace0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14efff920 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14eff0790 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11c869840 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14efffb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11d6baf10 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11d6bb550 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11d6bc500 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11c869ff0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11c86aba0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11c86a3e0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14efffd80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11c86bc40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11c86cad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14eff2090 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11c86cec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11c86d0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11c86da50 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11d6bc730 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x11d6bc960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x11d6bcb90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x11c86e660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x11c86f060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x11c86f290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x11d6bcdc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14eff3640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11d6bd870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x11d6bdc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x10f5b1740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f777b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x10f76f6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d6bde80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11c86fba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d6bd350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11c86fe20 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11944acf0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d6c0090 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d6bebd0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d6bee00 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d6bf490 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11c870f20 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11c871150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11c871380 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d6c0df0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11c8715b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11c872450 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11c872680 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d5044c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d6c3390 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d6c1560 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d6c1fd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d505290 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d505680 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d6c37e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11c872920 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11c872b50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11c872d80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11c873580 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11c8737b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11c873cd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11c874570 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11c8747a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11c874aa0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d6c2540 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d514d50 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d6c5e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11c874cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11c875b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11c876840 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d515140 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11c875d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11c877500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d515660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11c8780e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d515c10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d505f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d505bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d6c5720 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f40c1b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d5171d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d517ab0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d6c6f30 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f775d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f76e0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d517ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11944b1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11c878b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d517f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11c879720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d6c7160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d6c77b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d519040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x11c8791c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x11944c0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x11944e030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x11944cc40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x11d518910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x11d6c7e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d6c8080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d6c84b0 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 514 (with bs=512), 1 (with bs=1)\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: llama-2\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "# Initialize processor\n",
    "from utils import RiskEstimateProcessor\n",
    "\n",
    "processor = RiskEstimateProcessor(\"./models/llama-2-7b.Q4_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>estimation_measure</th>\n",
       "      <th>date</th>\n",
       "      <th>estimation</th>\n",
       "      <th>source_read_by_estimator</th>\n",
       "      <th>risk_category</th>\n",
       "      <th>other_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Anders Sandberg, adapting Denkenberger’s model</td>\n",
       "      <td>“Reduction in far future potential per year fr...</td>\n",
       "      <td>2018</td>\n",
       "      <td>~0.051%</td>\n",
       "      <td>Sort-of</td>\n",
       "      <td>nanotechnology</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Paul Christiano</td>\n",
       "      <td>Amount by which “really nailing” some portion ...</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.05</td>\n",
       "      <td>Yes</td>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>He says \"All of the numbers I’m going to give ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wells</td>\n",
       "      <td>Annual probability as of 2009 of extinction</td>\n",
       "      <td>2009</td>\n",
       "      <td>0.3-0.4%</td>\n",
       "      <td>No</td>\n",
       "      <td>total</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Pamlin &amp; Armstrong</td>\n",
       "      <td>\"Infinite impact\" from nuclear war within the ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>5e-05</td>\n",
       "      <td>No</td>\n",
       "      <td>nanotechnology</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Aggregate estimate of biorisk's contribution t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0002229877924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biorisk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Martin Rees</td>\n",
       "      <td>Odds that our present civilization on earth wi...</td>\n",
       "      <td>2003</td>\n",
       "      <td>≤50% (\"no better than fifty-fifty\")</td>\n",
       "      <td>No</td>\n",
       "      <td>total</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nanotechnology</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nanotechnology</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Paul Christiano</td>\n",
       "      <td>Amount by which “a marginal person” doing some...</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.005% (“one in 20,000 or something”)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>He says \"All of the numbers I’m going to give ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ben Garfinkel</td>\n",
       "      <td>\"AI causing an existential catastrophe in the ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>~0.1-1%</td>\n",
       "      <td>Yes</td>\n",
       "      <td>ai</td>\n",
       "      <td>Garfinkel was asked for his estimate during an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          estimator  \\\n",
       "64   Anders Sandberg, adapting Denkenberger’s model   \n",
       "107                                 Paul Christiano   \n",
       "9                                             Wells   \n",
       "66                               Pamlin & Armstrong   \n",
       "46                                              NaN   \n",
       "5                                       Martin Rees   \n",
       "56                                              NaN   \n",
       "57                                              NaN   \n",
       "108                                 Paul Christiano   \n",
       "28                                    Ben Garfinkel   \n",
       "\n",
       "                                    estimation_measure  date  \\\n",
       "64   “Reduction in far future potential per year fr...  2018   \n",
       "107  Amount by which “really nailing” some portion ...  2019   \n",
       "9          Annual probability as of 2009 of extinction  2009   \n",
       "66   \"Infinite impact\" from nuclear war within the ...  2015   \n",
       "46   Aggregate estimate of biorisk's contribution t...   NaN   \n",
       "5    Odds that our present civilization on earth wi...  2003   \n",
       "56                                                 NaN   NaN   \n",
       "57                                                 NaN   NaN   \n",
       "108  Amount by which “a marginal person” doing some...  2019   \n",
       "28   \"AI causing an existential catastrophe in the ...  2020   \n",
       "\n",
       "                                estimation source_read_by_estimator  \\\n",
       "64                                 ~0.051%                  Sort-of   \n",
       "107                                   0.05                      Yes   \n",
       "9                                 0.3-0.4%                       No   \n",
       "66                                   5e-05                       No   \n",
       "46                         0.0002229877924                      NaN   \n",
       "5      ≤50% (\"no better than fifty-fifty\")                       No   \n",
       "56                                     NaN                      NaN   \n",
       "57                                     NaN                      NaN   \n",
       "108  0.005% (“one in 20,000 or something”)                      Yes   \n",
       "28                                 ~0.1-1%                      Yes   \n",
       "\n",
       "      risk_category                                        other_notes  \n",
       "64   nanotechnology                                                NaN  \n",
       "107   miscellaneous  He says \"All of the numbers I’m going to give ...  \n",
       "9             total                                                NaN  \n",
       "66   nanotechnology                                                NaN  \n",
       "46          biorisk                                                NaN  \n",
       "5             total                                                NaN  \n",
       "56   nanotechnology                                                NaN  \n",
       "57   nanotechnology                                                NaN  \n",
       "108   miscellaneous  He says \"All of the numbers I’m going to give ...  \n",
       "28               ai  Garfinkel was asked for his estimate during an...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./data/pre-processed_data.csv')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   280 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11561.71 ms /   369 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 184 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 1 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   184 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    45 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8886.21 ms /   229 tokens\n",
      "Llama.generate: 40 prefix-match hit, remaining 156 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 2 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   156 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6990.90 ms /   210 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 162 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 3 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   162 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8110.84 ms /   214 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 182 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 4 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   182 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10710.54 ms /   252 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 170 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 5 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   170 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11469.60 ms /   231 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 181 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 6 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   181 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   24743.11 ms /   436 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 155 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 7 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   155 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7284.02 ms /   214 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 181 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 8 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   181 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   22508.14 ms /   436 tokens\n",
      "Llama.generate: 38 prefix-match hit, remaining 156 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 9 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   156 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   24457.82 ms /   411 tokens\n",
      "Llama.generate: 40 prefix-match hit, remaining 201 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 10 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7990.39 ms /   250 tokens\n",
      "Llama.generate: 38 prefix-match hit, remaining 179 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 11 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   179 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11386.09 ms /   235 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 156 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 12 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   156 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   23037.09 ms /   411 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 209 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 13 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8990.90 ms /   300 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 158 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 14 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   158 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19631.80 ms /   413 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 109 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 15 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   109 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   22214.25 ms /   364 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 227 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 16 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   227 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8056.46 ms /   276 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 459 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 17 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7126.63 ms /   474 tokens\n",
      "Llama.generate: 40 prefix-match hit, remaining 213 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 18 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7067.32 ms /   272 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 254 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 19 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   254 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10824.79 ms /   366 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 313 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 20 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   313 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8762.18 ms /   382 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 340 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 21 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   340 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8988.39 ms /   409 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 250 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 22 of 109\n",
      "Processed row 23 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   250 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9650.16 ms /   346 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 398 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 24 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   398 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10006.64 ms /   474 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 217 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 25 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    46 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6165.60 ms /   263 tokens\n",
      "Llama.generate: 49 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 26 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7303.72 ms /   342 tokens\n",
      "Llama.generate: 39 prefix-match hit, remaining 195 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 27 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19794.70 ms /   450 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 189 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 28 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6419.76 ms /   246 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 214 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 29 of 109\n",
      "Processed row 30 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9212.40 ms /   311 tokens\n",
      "Llama.generate: 40 prefix-match hit, remaining 309 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 31 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   309 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10255.84 ms /   405 tokens\n",
      "Llama.generate: 38 prefix-match hit, remaining 223 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 32 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7358.82 ms /   289 tokens\n",
      "Llama.generate: 39 prefix-match hit, remaining 320 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 33 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   320 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7916.42 ms /   377 tokens\n",
      "Llama.generate: 38 prefix-match hit, remaining 289 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 34 of 109\n",
      "Processed row 35 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   289 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7312.65 ms /   340 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 132 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 36 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18533.99 ms /   387 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 168 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 37 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   168 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    45 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5864.19 ms /   213 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 214 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 38 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7161.25 ms /   279 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 180 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 39 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   180 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19158.64 ms /   435 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 212 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 40 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10810.84 ms /   333 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 267 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 41 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   267 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   203 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   20251.60 ms /   470 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 181 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 42 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   181 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7514.08 ms /   252 tokens\n",
      "Llama.generate: 38 prefix-match hit, remaining 239 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 43 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   239 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   234 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19159.71 ms /   473 tokens\n",
      "Llama.generate: 56 prefix-match hit, remaining 183 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 44 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   183 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19356.90 ms /   438 tokens\n",
      "Llama.generate: 38 prefix-match hit, remaining 274 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 45 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   274 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   199 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   16409.17 ms /   473 tokens\n",
      "Llama.generate: 38 prefix-match hit, remaining 202 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 46 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   111 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9848.77 ms /   313 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 132 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 47 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18506.19 ms /   387 tokens\n",
      "Llama.generate: 168 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 48 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   256 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   15909.83 ms /   257 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 272 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 49 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   272 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   114 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12443.84 ms /   386 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 200 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 50 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5978.79 ms /   248 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 209 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 51 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10659.03 ms /   331 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 210 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 52 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7215.39 ms /   273 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 132 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 53 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18383.38 ms /   387 tokens\n",
      "Llama.generate: 168 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 54 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   256 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   16027.79 ms /   257 tokens\n",
      "Llama.generate: 168 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 55 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   256 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   15993.11 ms /   257 tokens\n",
      "Llama.generate: 168 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 56 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   256 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   15969.25 ms /   257 tokens\n",
      "Llama.generate: 168 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 57 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   256 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   15921.25 ms /   257 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 111 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 58 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18284.39 ms /   366 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 167 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 59 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   167 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    46 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5600.02 ms /   213 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 210 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 60 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11103.55 ms /   338 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 210 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 61 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   126 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10908.41 ms /   336 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 262 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 62 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   262 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8057.77 ms /   332 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 472 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 63 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     2 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7106.15 ms /   474 tokens\n",
      "Llama.generate: 40 prefix-match hit, remaining 172 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 64 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   172 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19284.75 ms /   427 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 65 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19228.03 ms /   461 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 197 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 66 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19199.25 ms /   452 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 132 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 67 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18888.00 ms /   387 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 111 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 68 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18851.05 ms /   366 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 167 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 69 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   167 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6607.14 ms /   228 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 204 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 70 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7016.13 ms /   264 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 198 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 71 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6190.74 ms /   246 tokens\n",
      "Llama.generate: 40 prefix-match hit, remaining 452 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 72 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7036.45 ms /   471 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 132 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 73 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19162.06 ms /   387 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 175 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 74 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   175 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19551.26 ms /   430 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 200 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 75 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19020.43 ms /   455 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 179 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 76 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   179 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    46 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5699.90 ms /   225 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 204 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 77 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19413.40 ms /   459 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 184 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 78 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   184 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6003.98 ms /   233 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 165 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 79 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   165 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19269.47 ms /   420 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 245 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 80 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   245 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   17847.16 ms /   474 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 132 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 81 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18478.82 ms /   387 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 268 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 82 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   268 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11612.88 ms /   391 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 394 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 83 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   394 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11458.26 ms /   474 tokens\n",
      "Llama.generate: 40 prefix-match hit, remaining 210 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 84 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9711.60 ms /   314 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 132 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 85 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18795.33 ms /   387 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 249 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 86 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   249 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8753.44 ms /   333 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 132 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 87 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18547.35 ms /   387 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 263 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 88 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   263 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7380.09 ms /   319 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 334 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 89 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   334 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10683.73 ms /   427 tokens\n",
      "Llama.generate: 40 prefix-match hit, remaining 262 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 90 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   262 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   129 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12147.56 ms /   391 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 294 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 91 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   294 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10790.16 ms /   398 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 271 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 92 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   271 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8370.25 ms /   340 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 176 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 93 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   176 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    46 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5965.82 ms /   222 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 204 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 94 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    47 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6287.56 ms /   251 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 262 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 95 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   262 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9150.93 ms /   347 tokens\n",
      "Llama.generate: 38 prefix-match hit, remaining 284 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 96 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   284 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12321.79 ms /   418 tokens\n",
      "Llama.generate: 38 prefix-match hit, remaining 164 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 97 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   164 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    47 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5865.82 ms /   211 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 331 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 98 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   331 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8671.34 ms /   397 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 346 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 99 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   346 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9205.65 ms /   417 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 182 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 100 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   182 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7324.80 ms /   254 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 293 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 101 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   293 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7167.02 ms /   344 tokens\n",
      "Llama.generate: 40 prefix-match hit, remaining 166 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 102 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   166 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19054.72 ms /   421 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 243 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 103 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   243 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7625.23 ms /   307 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 132 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 104 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18561.21 ms /   387 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 117 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 105 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   117 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18010.82 ms /   372 tokens\n",
      "Llama.generate: 37 prefix-match hit, remaining 214 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 106 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   20182.86 ms /   469 tokens\n",
      "Llama.generate: 38 prefix-match hit, remaining 199 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 107 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7510.26 ms /   267 tokens\n",
      "Llama.generate: 41 prefix-match hit, remaining 218 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 108 of 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   252 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19289.00 ms /   470 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 109 of 109\n"
     ]
    }
   ],
   "source": [
    "# Now process the whole total risk dataframe\n",
    "processed_df = processor.process_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>estimation_measure</th>\n",
       "      <th>date</th>\n",
       "      <th>estimation</th>\n",
       "      <th>source_read_by_estimator</th>\n",
       "      <th>risk_category</th>\n",
       "      <th>other_notes</th>\n",
       "      <th>century_probability</th>\n",
       "      <th>conversion_reasoning</th>\n",
       "      <th>conversion_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nanotechnology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Error in processing: No JSON object found in r...</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>GCR Conference</td>\n",
       "      <td>Human extinction by 2100 as a result of “the s...</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>Yes</td>\n",
       "      <td>biorisk</td>\n",
       "      <td>This is the median. Beard et al.'s appendix sa...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>The median estimate is 0.0005, which is the sa...</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Brian Tomasik</td>\n",
       "      <td>\"The probability of civilizational collapse to...</td>\n",
       "      <td>2015</td>\n",
       "      <td>\"Maybe ~20%?\"</td>\n",
       "      <td>Yes</td>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>Tomasik was asked about his estimate of the ch...</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>The probability of civilizational collapse to ...</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          estimator                                 estimation_measure  date  \\\n",
       "48              NaN                                                NaN   NaN   \n",
       "38   GCR Conference  Human extinction by 2100 as a result of “the s...  2008   \n",
       "103   Brian Tomasik  \"The probability of civilizational collapse to...  2015   \n",
       "\n",
       "        estimation source_read_by_estimator   risk_category  \\\n",
       "48             NaN                      NaN  nanotechnology   \n",
       "38          0.0005                      Yes         biorisk   \n",
       "103  \"Maybe ~20%?\"                      Yes   miscellaneous   \n",
       "\n",
       "                                           other_notes  century_probability  \\\n",
       "48                                                 NaN                  NaN   \n",
       "38   This is the median. Beard et al.'s appendix sa...               0.0005   \n",
       "103  Tomasik was asked about his estimate of the ch...               0.2000   \n",
       "\n",
       "                                  conversion_reasoning conversion_confidence  \n",
       "48   Error in processing: No JSON object found in r...                   low  \n",
       "38   The median estimate is 0.0005, which is the sa...                  high  \n",
       "103  The probability of civilizational collapse to ...                  high  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data to a CSV file.\n",
    "processed_df.to_csv('./data/risk_estimates_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 3 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6395.70 ms /   184 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5752.40 ms /   118 tokens\n",
      "Llama.generate: 23 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5754.19 ms /   124 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5802.94 ms /   128 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6006.84 ms /   138 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6039.43 ms /   144 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6104.46 ms /   128 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6864.81 ms /   134 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7424.60 ms /   120 tokens\n",
      "Llama.generate: 21 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5476.37 ms /   111 tokens\n",
      "Llama.generate: 23 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6100.57 ms /   120 tokens\n",
      "Llama.generate: 21 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5918.51 ms /   134 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5998.11 ms /   118 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6673.80 ms /   165 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6349.15 ms /   118 tokens\n",
      "Llama.generate: 74 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4134.00 ms /    65 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6470.99 ms /   123 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6287.60 ms /   131 tokens\n",
      "Llama.generate: 23 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6324.09 ms /   138 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6550.99 ms /   184 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6280.49 ms /   151 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6256.11 ms /   148 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6235.26 ms /   133 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6503.73 ms /   169 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6055.91 ms /   124 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6008.24 ms /   118 tokens\n",
      "Llama.generate: 33 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6369.38 ms /   122 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5936.57 ms /   121 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6245.69 ms /   132 tokens\n",
      "Llama.generate: 21 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6215.80 ms /   129 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6787.93 ms /   169 tokens\n",
      "Llama.generate: 23 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   105 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7050.45 ms /   168 tokens\n",
      "Llama.generate: 21 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6627.89 ms /   140 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6329.42 ms /   136 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6301.37 ms /   131 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6684.11 ms /   125 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7162.81 ms /   118 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6443.49 ms /   124 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6490.27 ms /   141 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6407.50 ms /   135 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   133 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7167.15 ms /   196 tokens\n",
      "Llama.generate: 24 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5919.08 ms /   154 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6201.00 ms /   152 tokens\n",
      "Llama.generate: 21 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6788.72 ms /   122 tokens\n",
      "Llama.generate: 79 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4352.79 ms /    65 tokens\n",
      "Llama.generate: 21 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6061.32 ms /   122 tokens\n",
      "Llama.generate: 21 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6157.40 ms /   148 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6051.51 ms /   118 tokens\n",
      "Llama.generate: 74 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3974.32 ms /    65 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   131 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6642.27 ms /   194 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5935.22 ms /   123 tokens\n",
      "Llama.generate: 24 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   129 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6714.33 ms /   192 tokens\n",
      "Llama.generate: 24 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6111.64 ms /   134 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5842.58 ms /   118 tokens\n",
      "Llama.generate: 74 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3964.11 ms /    65 tokens\n",
      "Llama.generate: 74 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3948.02 ms /    65 tokens\n",
      "Llama.generate: 74 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4006.12 ms /    65 tokens\n",
      "Llama.generate: 74 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3963.22 ms /    65 tokens\n",
      "Llama.generate: 74 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3967.32 ms /    65 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6158.87 ms /   129 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   139 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6712.36 ms /   202 tokens\n",
      "Llama.generate: 24 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   135 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6682.60 ms /   198 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6354.21 ms /   147 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5926.75 ms /   123 tokens\n",
      "Llama.generate: 23 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5882.50 ms /   121 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5886.24 ms /   124 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5798.02 ms /   123 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6007.51 ms /   118 tokens\n",
      "Llama.generate: 74 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4024.18 ms /    65 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6134.87 ms /   144 tokens\n",
      "Llama.generate: 24 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6128.48 ms /   131 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5956.71 ms /   123 tokens\n",
      "Llama.generate: 23 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6137.86 ms /   122 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6026.43 ms /   118 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6143.37 ms /   135 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5974.92 ms /   123 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6071.24 ms /   137 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5885.36 ms /   125 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6183.44 ms /   143 tokens\n",
      "Llama.generate: 24 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6099.76 ms /   131 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   112 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6518.58 ms /   175 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5897.71 ms /   118 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   154 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5878.86 ms /   206 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5997.68 ms /   121 tokens\n",
      "Llama.generate: 23 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   112 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6385.01 ms /   175 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5913.59 ms /   118 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6174.83 ms /   157 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5890.25 ms /   118 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6120.27 ms /   134 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   105 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6391.92 ms /   168 tokens\n",
      "Llama.generate: 23 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6247.54 ms /   136 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6154.69 ms /   137 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6153.33 ms /   141 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6171.48 ms /   130 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5931.87 ms /   121 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6474.28 ms /   165 tokens\n",
      "Llama.generate: 21 prefix-match hit, remaining 149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   149 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6751.99 ms /   212 tokens\n",
      "Llama.generate: 21 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5955.53 ms /   126 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6171.85 ms /   141 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6133.17 ms /   141 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6194.81 ms /   149 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6006.09 ms /   125 tokens\n",
      "Llama.generate: 23 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5897.35 ms /   118 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6202.58 ms /   139 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5879.68 ms /   118 tokens\n",
      "Llama.generate: 74 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4016.56 ms /    65 tokens\n",
      "Llama.generate: 20 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5910.24 ms /   121 tokens\n",
      "Llama.generate: 21 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6159.57 ms /   142 tokens\n",
      "Llama.generate: 24 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4344.99 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6181.45 ms /   135 tokens\n"
     ]
    }
   ],
   "source": [
    "# Validate results\n",
    "validated_total_risk_df = processor.validate_estimates(processed_df)\n",
    "\n",
    "# Basic analysis\n",
    "summary = validated_total_risk_df.groupby('risk_category').agg({\n",
    "    'century_probability': ['mean', 'median', 'std', 'count'],\n",
    "    'conversion_confidence': lambda x: (x == 'high').mean()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>estimation_measure</th>\n",
       "      <th>date</th>\n",
       "      <th>estimation</th>\n",
       "      <th>source_read_by_estimator</th>\n",
       "      <th>risk_category</th>\n",
       "      <th>other_notes</th>\n",
       "      <th>century_probability</th>\n",
       "      <th>conversion_reasoning</th>\n",
       "      <th>conversion_confidence</th>\n",
       "      <th>validation_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Rohin Shah</td>\n",
       "      <td>Probability of existential catastrophe due to ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Similar to 10%</td>\n",
       "      <td>Yes</td>\n",
       "      <td>ai</td>\n",
       "      <td>Rohin summarises the ecology / GPT perspective...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Error in processing: Requested tokens (537) ex...</td>\n",
       "      <td>low</td>\n",
       "      <td>If you have any questions, please contact the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Toby Ord</td>\n",
       "      <td>Existential catastrophe from “other anthropoge...</td>\n",
       "      <td>2020</td>\n",
       "      <td>~2% (~1 in 50)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>nanotechnology</td>\n",
       "      <td>See this post for some commentary: https://for...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>The estimate is for a 2% chance of an existent...</td>\n",
       "      <td>high</td>\n",
       "      <td>If you have any questions, please contact us a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Wei Dai</td>\n",
       "      <td>Expected fraction of total potential value tha...</td>\n",
       "      <td>2021</td>\n",
       "      <td>&gt;50%</td>\n",
       "      <td>Yes</td>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>\"What's your expectation of the fraction of to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Error in processing: Expecting ',' delimiter: ...</td>\n",
       "      <td>low</td>\n",
       "      <td>If you have any questions, please contact the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Toby Ord</td>\n",
       "      <td>Existential catastrophe from “engineered pande...</td>\n",
       "      <td>2020</td>\n",
       "      <td>~3% (~1 in 30)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>biorisk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "      <td>nan</td>\n",
       "      <td>high</td>\n",
       "      <td>If you have any questions, please contact the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Pablo Stafforini</td>\n",
       "      <td>Chance that \"Humans will go extinct within mil...</td>\n",
       "      <td>2015/2020</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Error in processing: No JSON object found in r...</td>\n",
       "      <td>low</td>\n",
       "      <td>If the conversion is valid, please move on to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Toby Ord</td>\n",
       "      <td>Existential catastrophe by 2120 as a result of...</td>\n",
       "      <td>2020</td>\n",
       "      <td>~10%</td>\n",
       "      <td>Yes</td>\n",
       "      <td>ai</td>\n",
       "      <td>Commentary here: https://forum.effectivealtrui...</td>\n",
       "      <td>0.100</td>\n",
       "      <td>The estimate is for a 10% chance of existentia...</td>\n",
       "      <td>high</td>\n",
       "      <td>If you have any questions, please contact me a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Will MacAskill</td>\n",
       "      <td>Existential risk in the 21st century</td>\n",
       "      <td>2019/2020</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Yes</td>\n",
       "      <td>total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Existential risk in the 21st century</td>\n",
       "      <td>high</td>\n",
       "      <td>If you have any questions, please contact the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Buck Shlegris</td>\n",
       "      <td>\"the probability of AI-induced existential ris...</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>ai</td>\n",
       "      <td>Note that Buck gave a 25 percentage point lowe...</td>\n",
       "      <td>0.050</td>\n",
       "      <td>The original estimate was 0.5, and the estimat...</td>\n",
       "      <td>high</td>\n",
       "      <td>If you find an issue, please fix it and then s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Matthew Barnett</td>\n",
       "      <td>Tthe risk of human extinction due to SETI</td>\n",
       "      <td>2022</td>\n",
       "      <td>~0.1-0.2%</td>\n",
       "      <td>Yes</td>\n",
       "      <td>miscellaneous</td>\n",
       "      <td>\"I currently think there is roughly a 99% chan...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>The estimate is very low, and the reasoning is...</td>\n",
       "      <td>low</td>\n",
       "      <td>If the output is \"invalid\", please describe th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Pamlin &amp; Armstrong</td>\n",
       "      <td>\"Infinite impact\" from AI within the next 100 ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>0-10%</td>\n",
       "      <td>No</td>\n",
       "      <td>ai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Error in processing: No JSON object found in r...</td>\n",
       "      <td>low</td>\n",
       "      <td>If you have any questions, please contact the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              estimator                                 estimation_measure  \\\n",
       "22           Rohin Shah  Probability of existential catastrophe due to ...   \n",
       "49             Toby Ord  Existential catastrophe from “other anthropoge...   \n",
       "91              Wei Dai  Expected fraction of total potential value tha...   \n",
       "37             Toby Ord  Existential catastrophe from “engineered pande...   \n",
       "102    Pablo Stafforini  Chance that \"Humans will go extinct within mil...   \n",
       "23             Toby Ord  Existential catastrophe by 2120 as a result of...   \n",
       "2        Will MacAskill               Existential risk in the 21st century   \n",
       "30        Buck Shlegris  \"the probability of AI-induced existential ris...   \n",
       "88      Matthew Barnett          Tthe risk of human extinction due to SETI   \n",
       "27   Pamlin & Armstrong  \"Infinite impact\" from AI within the next 100 ...   \n",
       "\n",
       "          date      estimation source_read_by_estimator   risk_category  \\\n",
       "22        2020  Similar to 10%                      Yes              ai   \n",
       "49        2020  ~2% (~1 in 50)                      Yes  nanotechnology   \n",
       "91        2021            >50%                      Yes   miscellaneous   \n",
       "37        2020  ~3% (~1 in 30)                      Yes         biorisk   \n",
       "102  2015/2020             0.1                      Yes   miscellaneous   \n",
       "23        2020            ~10%                      Yes              ai   \n",
       "2    2019/2020            0.01                      Yes           total   \n",
       "30        2020             0.5                      Yes              ai   \n",
       "88        2022       ~0.1-0.2%                      Yes   miscellaneous   \n",
       "27        2015           0-10%                       No              ai   \n",
       "\n",
       "                                           other_notes  century_probability  \\\n",
       "22   Rohin summarises the ecology / GPT perspective...                  NaN   \n",
       "49   See this post for some commentary: https://for...                0.001   \n",
       "91   \"What's your expectation of the fraction of to...                  NaN   \n",
       "37                                                 NaN                0.003   \n",
       "102                                                NaN                  NaN   \n",
       "23   Commentary here: https://forum.effectivealtrui...                0.100   \n",
       "2                                                  NaN                0.010   \n",
       "30   Note that Buck gave a 25 percentage point lowe...                0.050   \n",
       "88   \"I currently think there is roughly a 99% chan...                0.001   \n",
       "27                                                 NaN                  NaN   \n",
       "\n",
       "                                  conversion_reasoning conversion_confidence  \\\n",
       "22   Error in processing: Requested tokens (537) ex...                   low   \n",
       "49   The estimate is for a 2% chance of an existent...                  high   \n",
       "91   Error in processing: Expecting ',' delimiter: ...                   low   \n",
       "37                                                 nan                  high   \n",
       "102  Error in processing: No JSON object found in r...                   low   \n",
       "23   The estimate is for a 10% chance of existentia...                  high   \n",
       "2                 Existential risk in the 21st century                  high   \n",
       "30   The original estimate was 0.5, and the estimat...                  high   \n",
       "88   The estimate is very low, and the reasoning is...                   low   \n",
       "27   Error in processing: No JSON object found in r...                   low   \n",
       "\n",
       "                                      validation_notes  \n",
       "22   If you have any questions, please contact the ...  \n",
       "49   If you have any questions, please contact us a...  \n",
       "91   If you have any questions, please contact the ...  \n",
       "37   If you have any questions, please contact the ...  \n",
       "102  If the conversion is valid, please move on to ...  \n",
       "23   If you have any questions, please contact me a...  \n",
       "2    If you have any questions, please contact the ...  \n",
       "30   If you find an issue, please fix it and then s...  \n",
       "88   If the output is \"invalid\", please describe th...  \n",
       "27   If you have any questions, please contact the ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the validated data to a CSV file.\n",
    "validated_total_risk_df.to_csv('./data/risk_estimates_with_validation_remarks.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
