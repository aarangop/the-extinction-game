estimator,estimation_measure,date,estimation,source_read_by_estimator,risk_category,other_notes,source,estimation_numeric
Toby Ord,“Total existential risk” by 2120,2020,~17% (~1 in 6),Yes,total,"Ord writes: ""Don’t take these numbers to be completely objective. [...] And don’t take the estimates to be precise. Their purpose is to show the right order of magnitude, rather than a more precise probability.""

This estimate already incorporates Ord's expectation that people will start taking these risks more seriously in future. For his ""business as usual"" estimate, see the conditional estimates sheet.",The Precipice,
GCR Conference,“Overall risk of extinction prior to 2100”,2008,0.19,Yes,total,This is the median. The report about these estimates also plots the results for each question “with individual response distributions visible” in Appendix A.,https://www.fhi.ox.ac.uk/reports/2008-1.pdf,0.19
Will MacAskill,Existential risk in the 21st century,2019/2020,0.01,Yes,total,,https://80000hours.org/podcast/episodes/will-macaskill-paralysis-and-hinge-of-history/#transcript,0.01
"Ben Todd or 80,000 Hours",Extinction risk “in the next century”,2017,Probably at or above 3%,Yes,total,,https://80000hours.org/articles/extinction-risk/,
John Leslie,Risk of extinction over the next five centuries,1996,At or above 30%,No,total,"""The probability of the human race avoiding extinction for the next five centuries is encouragingly high, perhaps as high as 70 percent”","Leslie, J. (1996). The End of the World: the science and ethics of human extinction.",
Martin Rees,Odds that our present civilization on earth will survive to the end of the present century,2003,"≤50% (""no better than fifty-fifty"")",No,total,,"Rees, M. J. (2003). Our final century. Basic Books",
Metaculus responders,"""there be zero living humans on planet earth on January 1, 2100""",,Median: 1%. Mean: 8%.,No,total,That median and mean is as of 3rd July 2019.,https://www.metaculus.com/questions/578/human-extinction-by-2100/,
Nick Bostrom,“existential disaster will do us in”,2002,Probably at or above 25%,No,total,,https://www.nickbostrom.com/existential/risks.html,
Gott III,"The prior probability that ""humanity will cease to exist before 5,100 years or thrive beyond 7.8 million years"" (quote from Beard et al.'s appendix)",1993,5%.,No,total,,"Gott III, J. R. (1993). Implications of the Copernican principle for our future prospects. Nature, 363, 315-319. ",
Wells,Annual probability as of 2009 of extinction,2009,0.3-0.4%,No,total,,"Wells, W. (2009) Human survivability. In: Apocalypse When?. Springer Praxis Books. Praxis https://doi.org/10.1007/978-0-387-09837-1_5 ",
Simpson,“Humanity’s prognosis for the coming century is well approximated by a global catastrophic risk of 0.2% per year.”,2016,0.002,No,total,"Beard et al. seem to imply this is about extinction, but the quote suggests it's about ""global catastrophic risk"".","Simpson, F. (2016). Apocalypse now? Reviving the Doomsday argument. arXiv preprint arXiv:1611.03072. ",0.002
Toby Ord,“Humanity avoids every existential catastrophe and eventually fulfils its potential: achieving something close to the best future open to us”,2020,50% (~1 in 2),Yes,total,,The Precipice,
Frank Tipler,"""Personally, I now think we humans will be wiped out this century""",2019?,,No,total,,"William Poundstone, The Doomsday Calculation, p. 259",
Ozzie Gooen,"""sentient life will survive for at least billions of years""",2020,>20%,Yes,total,"""I think it's fairly likely(>20%) that sentient life will survive for at least billions of years; and that there may be a fair amount of lock-in, so changing the trajectory of things could be great.""",https://forum.effectivealtruism.org/posts/MSYhEatxkEfg46j3D/the-case-of-the-missing-cause-prioritisation-research?commentId=iWkoScDxocaAJE4Jg,
"I will not report the number from Stern Review: The economics of climate change, as it was not actually meant as an estimate.",,,,,total,,,
,,,,,total,,,
Buck Shlegris,"""Overall P(doom)"" (from context, this seems to be a prediction just about P(doom) from AI, or seems to be expecting almost 100% of P(doom) is accounted for by AI, but that's not specified)",2023,0.25,Yes,ai,"""My overall P(doom) is like 25% (though it fluctuates wildly""",https://youtu.be/YTlrPeikoyw?t=1788,0.25
Bensinger's survey of 44 people working on long-term AI risk,"""How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of humanity not doing enough technical AI safety research?""",2021,"Mean: ~30%
Median: ~20%",Yes,ai,"""I sent a two-question survey to ~117 people working on long-term AI risk [...] 44 people responded (~38% response rate). In all cases, these represent the views of specific individuals, not an official view of any organization. Since some people's views may have made them more/less likely to respond, I suggest caution in drawing strong conclusions from the results below. Another reason for caution is that respondents added a lot of caveats to their responses (see the anonymized spreadsheet)"".
Bensinger sent the survey directly to ""MIRI's research team, and people who recently left OpenAI (mostly people suggested by Beth Barnes of OpenAI). I [also] sent it to five other groups through org representatives (who I asked to send it to everyone at the org ""who researches long-term AI topics, or who has done a lot of past work on such topics""): OpenAI, the Future of Humanity Institute (FHI), DeepMind, the Center for Human-Compatible AI (CHAI), and Open Philanthropy""
Further details and caveats in the post.",https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results,
Bensinger's survey of 44 people working on long-term AI risk,"""How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of AI systems not doing/optimizing what the people deploying them wanted/intended?""",2021,"Mean: ~40%
Median: ~30%",Yes,ai,,https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results,
Joe Carlsmith,"Existential catastrophe by 2070 from scenarios involving ""power-seeking AI"" and some specific conditions being met",2021,~5%,No,ai,"""My current, highly-unstable, subjective estimate is that there is a ~5% percent chance of existential catastrophe by 2070 from scenarios in which (1)-(6) are true"".
This comes from a draft report. By the time you're reading this, Carlsmith may have updated his estimate without me updating what's shown here.",https://forum.effectivealtruism.org/posts/78NoGoRitPzeT8nga/draft-report-on-existential-risk-from-power-seeking-ai,
Survey respondents,Probability of “doom” before 2070 due to the type of problem discussed in Carlsmith's report on existential risk from power-seeking AI,2021,Survey answers ranged from <1% to >50%,No,ai,"My only knowledge of this survey comes from a footnote in Karnofsky's ""Most Important Century"" series which says: ""A more detailed, private survey done for this report, asking about the probability of “doom” before 2070 due to the type of problem discussed in the report, got answers ranging from <1% to >50%. In my opinion, there are very thoughtful people who have seriously considered these matters at both ends of that range.""
2021 is just my guess as to when the survey would've been sent out.",,
Beth Barnes,"""the [AI] persuasion problem as distinct from the alignment problem""",2021,10x smaller than the alignment problem,Yes,ai,"""Overall I think this threat is significantly smaller than more standard alignment failure scenarios (maybe 10x smaller), but comparable enough that interventions could be well worthwhile if they're fairly tractable. [...] How to divide the space is a bit confusing here; I’d say something like ‘the persuasion problem as distinct from the alignment problem’ is 10x smaller, but in fact there’s some overlap, so it might also be reasonable to say something like ‘¼ of alignment-failure-esque xrisk scenarios will have a significant societal-scale persuasion component’, and almost all will have some deception component (and the fact that it’s hard to train your AI to be honest with you will be a key problem)"". See the source for more on what Barnes means by ""the persuasion problem"". ",https://www.alignmentforum.org/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion,
Rohin Shah,"Probability of existential catastrophe due to the risks highlighted by the ecology / GPT, and ARCHES perspectives",2020,Similar to 10%,Yes,ai,"Rohin summarises the ecology / GPT perspectives from Dafoe (2020) and the ARCHES paper, then writes ""I’ve estimated (AN #80) a 10% chance of existential catastrophe via a failure of intent alignment, absent intervention from longtermists to address intent alignment. Estimates vary quite a lot, even among people who have thought about the problem a lot; I’ve heard as low as < 1% and as high as 80% (though these usually don’t assume “no intervention from longtermists”). It’s harder to estimate the importance of structural risks and extinction risks highlighted in the two summaries above, but the arguments in the previous two posts seem reasonably compelling and I think I’d be inclined to assign a similar importance to it (i.e. similar probability of causing an existential catastrophe). Note that this means I’m disagreeing with Critch: he believes that we are far more likely to go extinct through effects unique to multi-multi dynamics; in contrast I find the argument less persuasive because we do have governance, regulations, national security etc. that would already be trying to mitigate issues that arise in multi-multi contexts, especially things that could plausibly cause extinction.""
This wasn't exactly a quantitative estimate, so is arguably out of scope for this database, but seemed worth including.",https://www.alignmentforum.org/posts/8eX8DJctsACtR2sfX/an-118-risks-solutions-and-prioritization-in-a-world-with,
Toby Ord,Existential catastrophe by 2120 as a result of “unaligned AI”,2020,~10%,Yes,ai,Commentary here: https://forum.effectivealtruism.org/posts/Z5KZ2cui8WDjyF6gJ/some-thoughts-on-toby-ord-s-existential-risk-estimates#What_types_of_catastrophe_are_included_in_the__Unaligned_AI__estimate_,The Precipice,
Rob Wiblin,"""the risk of a severe, even existential catastrophe caused by machine intelligence within the next 100 years""",2017,"""something like 10%""",Yes,ai,"It's unclear what's being estimated here, since ""severe catastrophe"" and ""existential catastrophe"" are extremely different bars. 
I think there's also no indication of how this estimate was arrived at. But this comment is made regarding the article as a whole: ""This profile is based on interviews with: Professor Nick Bostrom at the University of Oxford, the author of Superintelligence; an anonymous leading professor of computer science; Jaan Tallinn, one of the largest donors in the space and the co-founder of Skype; Jan Leike, a machine learning researcher now at DeepMind; Miles Brundage, an AI policy researcher at the Future of Humanity Institute at Oxford University; Nate Soares, the Executive Director of the Machine Intelligence Research Institute; Daniel Dewey, who works full-time finding researchers and funding opportunities in the field; and several other researchers in the area. We also read advice from David Krueger, a Machine Learning PhD student.""","""Positively shaping the development of artificial intelligence""",
Global Catastrophic Risk Conference,Human extinction by 2100 as a result of “superintelligent AI”,2008,0.05,Yes,ai,"This is the median. Beard et al.'s appendix says ""Note that for these predictions no time frame was given."" I think that that's incorrect, based on phrasings in the original source, but I'm not certain.",https://www.fhi.ox.ac.uk/reports/2008-1.pdf,0.05
Survey of “AI experts”,“Extremely bad (e.g. extinction)” long-run impact on humanity from “high-level machine intelligence”,2017,0.05,Yes,ai,"The report's authors discuss potential concerns around non-response bias and the fact that “NIPS and ICML authors are representative of machine learning but not of the field of artificial intelligence as a whole”. There was also evidence of apparent inconsistencies in estimates of AI timelines as a result of small changes to how questions were asked, providing further reason to wonder how meaningful these experts’ predictions were. https://web.archive.org/web/20171030220008/https://aiimpacts.org/some-survey-results/",https://arxiv.org/abs/1705.08807,0.05
Pamlin & Armstrong,"""Infinite impact"" from AI within the next 100 years, which ""refers to the state where civilization collapses and does not recover, or a situation where all human life ends"" (according to Beard et al.)",2015,0-10%,No,ai,,"Pamlin, D. & Armstrong, S. (2015). Global Challenges: 12 Risks that Threaten Human Civilisation, Global Challenges Foundation.   ",
Ben Garfinkel,"""AI causing an existential catastrophe in the next century""",2020,~0.1-1%,Yes,ai,"Garfinkel was asked for his estimate during an AMA, and replied ""I currently give it something in the .1%-1% range.""",https://forum.effectivealtruism.org/posts/7gxtXrMeqw78ZZeY9/ama-or-discuss-my-80k-podcast-episode-ben-garfinkel-fhi?commentId=uxiKooRc6d7JpjMSg,
Rohin Shah,"Chance that AI, through “adversarial optimization against humans only”, will cause existential catastrophe",2020,~5%,Yes,ai,"This is my interpretation of some comments that may not have been meant to be taken very literally. Elsewhere, Rohin noted that this was “[his] opinion before updating on other people's views"": https://forum.effectivealtruism.org/posts/tugs9KQyNqi4yRTsb/does-80-000-hours-focus-too-much-on-ai-risk#ZmtPji3pQaZK7Y4FF I think he updated this in 2020 to ~9%, due to pessimism about discontinuous scenarios: https://www.lesswrong.com/posts/TdwpN484eTbPSvZkm/rohin-shah-on-reasons-for-ai-optimism?commentId=n577gwGB3vRpwkBmj Rohin also discusses his estimates here: https://futureoflife.org/2020/04/15/an-overview-of-technical-ai-alignment-in-2018-and-2019-with-buck-shlegeris-and-rohin-shah/
In 2021, Rohin similarly said ""Usually when I’m asked to predict this, the actual prediction I give is, probability that we go extinct due to an intent alignment failure, and then depending on the situation I will either condition on… I will either make that unconditional, so that includes all of the things that people will do to try to prevent that from happening. Or, I make it conditional, on the long-termist community doesn’t do anything, or vanishes or something. But even in that world, there’s still… Everyone who’s not a long-termist, who can still prevent that from happening, which I really do expect them to do, and then I think I give my cached answer, on both of those is like 5% and 10% respectively, which I think is probably the numbers I gave you. If I actually sat down and try to like come up with a probability, I would probably come up with something different this time, but I have not done that, and I’m way too anchored on those previous estimates, to be able to give you a new estimate this time.""",https://www.lesswrong.com/posts/TdwpN484eTbPSvZkm/rohin-shah-on-reasons-for-ai-optimism,
Buck Shlegris,"""the probability of AI-induced existential risk"" (but from context, I believe this actually meant the probability of an AI-induced existential catastrophe)",2020,0.5,Yes,ai,"Note that Buck gave a 25 percentage point lower estimate for seemingly the same question in 2023, as also captured in this spreadsheet. ",https://futureoflife.org/2020/04/15/an-overview-of-technical-ai-alignment-in-2018-and-2019-with-buck-shlegeris-and-rohin-shah/,0.5
James Fodor,Existential risk from unaligned AI over the coming 100 years,2020,0.0005,Yes,ai,"This was a direct response to Ord's estimate. It focuses on one pathway to x-risk from AI, not all pathways (e.g., not AI misuse or risks from competition between powerful AIs). ""These estimates should not be taken very seriously. I do not believe we have enough information to make sensible quantitative estimates about these eventualities. Nevertheless, I present my estimates largely in order to illustrate the extent of my disagreement with Ord’s estimates, and to illustrate the key considerations I examine in order to arrive at an estimate."" In comments on the source, Rohin Shah critiques some of the inputs to this estimate, and provides his own, substantially higher estimates.",Critical Review of 'The Precipice': A Reassessment of the Risks of AI and Pandemics,0.0005
Stuart Armstrong,Existential risk from AI,2020,5-30%,Yes,ai,"""I put the probability that [AI/AGI] is an existential risk roughly in the 30% to 5% range, depending on how the problem is phrased."" I assume he means the probability of existential catastrophe from AI/AGI, not the probability that AI/AGI poses an existential risk. ",https://youtu.be/WLXuZtWoRcE?t=1229,
Stuart Armstrong,Chance of humanity not surviving AI,2014,"50, 40, or 33%",Yes,ai,"Stated verbally during an interview. Not totally clear precisely what was being estimated (e.g. just extinction, or existential catastrophe more broadly?). He noted ""This number fluctuates a lot"". He indicated he thought we had a 2/3 chance of surviving, then said he'd adjust to 50%, which is his number for an ""actually superintelligent"" AI, whereas for ""AI in general"" it'd be 60%. This is notably higher than his 2020 estimate, implying either that he updated towards somewhat more ""optimism"" between 2014 and 2020, or that one or both of these estimates don't reflect stable beliefs.",https://www.youtube.com/watch?v=i4LjoJGpqIY& (from 39:40),
Paul Christiano,Amount by which risk of failure to align AI (using only a narrow conception of alignment) reduces the expected value of the future,2019,~10%,Yes,ai,"He also says ""I made up 10%, it’s kind of a random number."" And ""All of the numbers I’m going to give are very made up though. If you asked me a second time you’ll get all different numbers.""

Christiano gave a similar estimate in a 2021 podcast interview: 
""Daniel Filan: [...] the first topic I want to talk about is this idea that AI might pose some kind of existential threat or an existential risk, and there’s this common definition of existential risk, which is a risk of something happening that would incapacitate humanity and limit its possibilities for development, incredibly drastically in a way comparable to human extinction, such as human extinction. Is that roughly the definition you use?

Paul Christiano: Yeah. I think I don’t necessarily have a bright line around giant or drastic drops versus moderate drops. I often think in terms of the expected fraction of humanity’s potential that is lost. But yeah, that’s basically what I think of it. Anything that could cause us not to fulfill some large chunk of our potential. I think of AI in particular, a failure to align AI maybe makes the future, in my guess 10% or 20% worse, or something like that, in expectation. And that makes it one of the worst things. I mean, not the worst, that’s a minority of all of our failure to fall short of our potential, but it’s a lot of failure to fall short of our potential. You can’t have that many 20% hits before you’re down to no potential left.

Daniel Filan: Yeah. When you say a 10% or 20% hit to human potential in expectation, do you mean if we definitely failed to align AI or do you mean we may or may not fail to align AI and overall that uncertainty equates to a 20%, or 10% to 20% hit?

Paul Christiano: Yeah, that’s unconditionally. So I think if you told me we definitely mess up alignment maximally then I’m more like, oh, now I are looking at a pretty big, close to 100% drop. I wouldn’t go all the way to 100.""",https://aiimpacts.org/conversation-with-paul-christiano/,
Jaan Tallinn,"""likelihood of an existential catastrophe happening this century"" (maybe just from AI?)",2020,33-50%,Yes,ai,"This comes from a verbal interview (from the 14:14 mark). The interview was focused on AI, and this estimate may have been as well. Tallinn said he's not very confident, but is fairly confident his estimate would be in double-digits, and then said ""two obvious Schelling points"" are 33% or 50%, so he'd guess somewhere in between those. Other comments during the interview seem to imply Tallinn is either just talking about extinction risk or thinks existential risk happens to be dominated by extinction risk.",https://youtu.be/aFAI8itZCGk?t=854,
,,,,,ai,,,
Toby Ord,Existential catastrophe from “engineered pandemics” by 2120,2020,~3% (~1 in 30),Yes,biorisk,,The Precipice,
GCR Conference,Human extinction by 2100 as a result of “the single biggest natural pandemic”,2008,0.0005,Yes,biorisk,"This is the median. Beard et al.'s appendix says ""Note that for these predictions no time frame was given."" I think that that's incorrect, based on phrasings in the original source, but I'm not certain.",https://www.fhi.ox.ac.uk/reports/2008-1.pdf,0.0005
Toby Ord,Existential catastrophe from “‘naturally’ arising pandemics” by 2120,2020,"~0.01% (~1 in 10,000)",Yes,biorisk,,The Precipice,
GCR Conference,Human extinction by 2100 as a result of “single biggest engineered pandemic”,2008,0.02,Yes,biorisk,"This is the median. Beard et al.'s appendix says ""Note that for these predictions no time frame was given."" I think that that's incorrect, based on phrasings in the original source, but I'm not certain.",https://www.fhi.ox.ac.uk/reports/2008-1.pdf,0.02
Millet & Snyder-Beattie,The annual probability of an existential catastrophe arising from a global pandemic,2017,0.008% to 0.0000016% (between 8 x 10-5 and 1.6 x 10-8),No,biorisk,"The fact that there's a separate estimate from the same source for biowarfare and bioterrorism suggests to me that  this is meant to be an estimate of the risk from a natural pandemic only. But I'm not sure. This might also include ""accidental"" release of a bioengineered pathogen.","Millett, P., & Snyder-Beattie, A. (2017). Existential risk and cost-effective biosecurity. Health security, 15(4), 373-383. https://doi.org/10.1089/hs.2017.0028  ",
Millet & Snyder-Beattie,The annual probability of an existential catastrophe arising from biowarfare or bioterrorism,2017,0.00019% (0.0000019),No,biorisk,,"Millett, P., & Snyder-Beattie, A. (2017). Existential risk and cost-effective biosecurity. Health security, 15(4), 373-383. https://doi.org/10.1089/hs.2017.0028  ",
Pamlin & Armstrong,"""Infinite impact"" from a global pandemic within the next 100 years, which ""refers to the state where civilization collapses and does not recover, or a situation where all human life ends"" (according to Beard et al.)",2015,1e-06,No,biorisk,"The fact that there's a separate estimate from the same source for ""synthetic biology"" suggests to me that this is meant to be an estimate of the risk from a natural pandemic only.","Pamlin, D. & Armstrong, S. (2015). Global Challenges: 12 Risks that Threaten Human Civilisation, Global Challenges Foundation.   ",1e-06
Pamlin & Armstrong,"""Infinite impact"" from ""synthetic biology"" within the next 100 years, which ""refers to the state where civilization collapses and does not recover, or a situation where all human life ends"" (according to Beard et al.)",2015,1e-06,No,biorisk,,"Pamlin, D. & Armstrong, S. (2015). Global Challenges: 12 Risks that Threaten Human Civilisation, Global Challenges Foundation.   ",1e-06
James Fodor,Extinction risk from engineered pandemics over the coming 100 years,2020,2e-06,Yes,biorisk,"This was a direct response to Ord's estimate, although this estimate is of extinction risk rather than existential risk. ""These estimates should not be taken very seriously. I do not believe we have enough information to make sensible quantitative estimates about these eventualities. Nevertheless, I present my estimates largely in order to illustrate the extent of my disagreement with Ord’s estimates, and to illustrate the key considerations I examine in order to arrive at an estimate."" In comments on the source, Will Bradshaw critiques some of the inputs to this estimate.",Critical Review of 'The Precipice': A Reassessment of the Risks of AI and Pandemics,2e-06
,"Aggregate estimate of biorisk's contribution to total x-risk by 2120 (very naive geometric mean, quickly and roughly extrapolating each estimate above into ""contribution to total x-risk by 2120"")",,0.0002229877924,,biorisk,,,0.0002229877924
,,,,,biorisk,,,
,,,,,nanotechnology,,,
Toby Ord,Existential catastrophe from “other anthropogenic risks” (which includes but is not limited to nanotechnology) by 2120,2020,~2% (~1 in 50),Yes,nanotechnology,See this post for some commentary: https://forum.effectivealtruism.org/posts/Z5KZ2cui8WDjyF6gJ/my-thoughts-on-toby-ord-s-existential-risk-estimates#_Unforeseen__and__other__anthropogenic_risks__Surprisingly_risky_,The Precipice,
Pamlin & Armstrong,"""Infinite impact"" from nanotechnology within the next 100 years, which ""refers to the state where civilization collapses and does not recover, or a situation where all human life ends"" (according to Beard et al.)",2015,0.0001,No,nanotechnology,,"Pamlin, D. & Armstrong, S. (2015). Global Challenges: 12 Risks that Threaten Human Civilisation, Global Challenges Foundation.   ",0.0001
GCR Conference,Human extinction by 2100 as a result of “molecular nanotech weapons”,2008,0.05,Yes,nanotechnology,"This is the median. Beard et al.'s appendix says ""Note that for these predictions no time frame was given."" I think that that's incorrect, based on phrasings in the original source, but I'm not certain.",https://www.fhi.ox.ac.uk/reports/2008-1.pdf,0.05
GCR Conference,Human extinction by 2100 as a result of “the single biggest nanotech accident”,2008,0.005,Yes,nanotechnology,"This is the median. Beard et al.'s appendix says ""Note that for these predictions no time frame was given."" I think that that's incorrect, based on phrasings in the original source, but I'm not certain.",https://www.fhi.ox.ac.uk/reports/2008-1.pdf,0.005
,,,,,nanotechnology,,,
,,,,,nanotechnology,,,
,,,,,nanotechnology,,,
,,,,,nanotechnology,,,
,,,,,nanotechnology,,,
Nuclear,,,,,nanotechnology,,,
Toby Ord,Existential catastrophe from nuclear war by 2120,2020,~0.1% (~1 in 1000),Yes,nanotechnology,,The Precipice,
GCR Conference,Human extinction by 2100 as a result of “nuclear wars”,2008,0.01,Yes,nanotechnology,"This is the median. Beard et al.'s appendix says ""Note that for these predictions no time frame was given."" I think that that's incorrect, based on phrasings in the original source, but I'm not certain.",https://www.fhi.ox.ac.uk/reports/2008-1.pdf,0.01
GCR Conference,Human extinction by 2100 as a result of “acts of nuclear terrorism”,2008,0.0003,Yes,nanotechnology,"This is the median. Beard et al.'s appendix says ""Note that for these predictions no time frame was given."" I think that that's incorrect, based on phrasings in the original source, but I'm not certain.",https://www.fhi.ox.ac.uk/reports/2008-1.pdf,0.0003
"Ben Todd or 80,000 Hours",The chance of “a civilization-ending nuclear war” (or one that causes extinction?) in “the next century”,2017,Probably at or above 0.3%,Yes,nanotechnology,"It’s also worth noting that Todd uses the phrases “permanently end civilization” and “civilization-ending nuclear war” here, but the article and some other estimates in it are focused on extinction, rather than other scenarios like permanent collapse. I’m thus not sure precisely what scenarios Todd is estimating (or broadly discussing) the risk of.",https://80000hours.org/articles/extinction-risk/,
Dave Denkenberger or ALLFED,“Reduction in far future potential per year from [the risk of a?] full scale nuclear war”,2018,~0.29%,Sort-of,nanotechnology,"There's some relevant info/discussion in these three sources (especially the first two): https://forum.effectivealtruism.org/posts/CcNY4MrT5QstNh4r7/cost-effectiveness-of-foods-for-global-catastrophes-even https://80000hours.org/podcast/episodes/david-denkenberger-allfed-and-feeding-everyone-no-matter-what/ https://forum.effectivealtruism.org/posts/D8t5TtSemqcGa4g7Y/my-open-for-feedback-donation-plans#1__Should_I_donate_to_ALLFED_ Denkenberger's model includes an estimate of the annual probability of nuclear war, based on Barrett 2013, and an estimate of the impacts a nuclear war would have if it occurred, based on a survey he sent to “31 GCR researchers”, which “got seven responses (including [Denkenberger himself])”. He multiplies these figures together to get the “Reduction in far future potential per year from full scale nuclear war” (which I believe should say “from the risk of a full scale nuclear war”). The estimate for that is ~0.29%.
",https://www.getguesstimate.com/models/11762,
"Anders Sandberg, adapting Denkenberger’s model",“Reduction in far future potential per year from [the risk of a?] full scale nuclear war”,2018,~0.051%,Sort-of,nanotechnology,,https://www.getguesstimate.com/models/11691,
Alexey Turchin,"The risk of extinction due to the consequences of nuclear war, or as a result of a ‘Doomsday machine’, in the 21st century",2008,In the order of 1%,No,nanotechnology,Beard et al.'s appendix makes it seem somewhat unclear what this is about or how to interpret. See that appendix for details.,"Turchin, A. V. (2008) Structure of the global catastrophe. Risks of human extinction in the XXI century lulu.com",
Pamlin & Armstrong,"""Infinite impact"" from nuclear war within the next 100 years, which ""refers to the state where civilization collapses and does not recover, or a situation where all human life ends"" (according to Beard et al.)",2015,5e-05,No,nanotechnology,,"Pamlin, D. & Armstrong, S. (2015). Global Challenges: 12 Risks that Threaten Human Civilisation, Global Challenges Foundation.   ",5e-05
,,,,,nanotechnology,,,
Toby Ord,Existential catastrophe from climate change by 2120,2020,~0.1% (~1 in 1000),Yes,climate_change,,The Precipice,
Roman Duda,Amount by which “the future potential of humanity” is reduced due to the risk of “extreme climate change (>5ºC) in the next 100 years” ,2016,~0.1-2%,Yes,climate_change,I multiplied 0.5-10% by ~20%. I may be misinterpreting his statement.,https://80000hours.org/problem-profiles/climate-change/,
Pamlin & Armstrong,"""Infinite impact"" from climate change within the next 100 years, which ""refers to the state where civilization collapses and does not recover, or a situation where all human life ends"" (according to Beard et al.)",2015,0.0001,No,climate_change,,"Pamlin, D. & Armstrong, S. (2015). Global Challenges: 12 Risks that Threaten Human Civilisation, Global Challenges Foundation.   ",0.0001
Toby Ord,"Amount by which existential risk till 2120 would decrease if ""we could just somehow have the next century but make it so that climate change wasn’t an issue""",2020,0.1-1 percentage points,Yes,climate_change,"""Toby Ord: [...] I think that if we imagine a world, if we could just somehow have the next century but make it so that climate change wasn’t an issue. All of the dedicated altruists who are working on fighting climate change could then work on other things and global international tensions on this would go down and so nations could spend their “altruistic international corporation” kind of budget on something else. So I do think that that could actually be quite helpful. As to how big it is as a risk factor, my guess would be somewhere between… these are very rough kind of guesses, between about 0.1% and 1%. So maybe a bit bigger as a risk factor, but not an order of magnitude. Probably not a whole order of magnitude bigger."" 

It's possible he means 0.1-1% *of the total risk level over the next 100 years*, which would correspond to ~0.017-0.17 percentage points. But he estimates climate change itself/directly poses a 1/1000 risk, and says that it's bigger as a risk factor, which only makes sense if he meant 0.1-1 percentage points of risk from climate change as a risk factor. ",https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/#transcript,
,,,,,climate_change,,,
Toby Ord,Existential catastrophe from supervolcanic eruption by 2120,2020,"~0.01% (~1 in 10,000)",Yes,natural_risks,,The Precipice,
Pamlin & Armstrong,"""Infinite impact"" from a super-volcano within the next 100 years, which ""refers to the state where civilization collapses and does not recover, or a situation where all human life ends"" (according to Beard et al.)",2015,3e-07,No,natural_risks,,"Pamlin, D. & Armstrong, S. (2015). Global Challenges: 12 Risks that Threaten Human Civilisation, Global Challenges Foundation.   ",3e-07
Toby Ord,Existential catastrophe from asteroid or comet impact by 2120,2020,"~0.0001% (~1 in 1,000,000)",Yes,natural_risks,,The Precipice,
Pamlin & Armstrong,"""Infinite impact"" from ""a major asteroid impact"" within the next 100 years, which ""refers to the state where civilization collapses and does not recover, or a situation where all human life ends"" (according to Beard et al.)",2015,1.3e-06,No,natural_risks,,"Pamlin, D. & Armstrong, S. (2015). Global Challenges: 12 Risks that Threaten Human Civilisation, Global Challenges Foundation.   ",1.3e-06
Toby Ord,Existential catastrophe from stellar explosion by 2120,2020,"~0.0000001% (~1 in 1,000,000,000)",Yes,natural_risks,,The Precipice,
Toby Ord,“Total natural [existential] risk” by 2120,2020,"~0.01% (~1 in 10,000)",Yes,natural_risks,,The Precipice,
"Snyder-Beattie, Ord, & Bonsall","The probability that humanity goes extinct from natural causes in any given year, using only the information that Homo sapiens has existed at least 200,000 years.",2019,"Almost guaranteed to be below 0.007% (1 in 14000), likely less than 0.001%, likely less than 0.0001% if we use the track record of survival of the genus Homo",No,natural_risks,,https://www.nature.com/articles/s41598-019-47540-7,
,,,,,natural_risks,,,
Toby Ord,Amount by which existential risk till 2120 would decrease if there definitely wouldn't be a great power war during that time,2020,"~1.7 percentage points (""something like a tenth"" of the ~1/6 total risk over that time)",Yes,war,"""Consider your own estimate of how much existential risk there is over the next hundred years. How much of this would disappear if you knew that the great powers would not go to war with each other over that time? It is impossible to be precise, but I’d imagine that an appreciable fraction would disappear - something like a tenth of the existential risk over that time.""",The Precipice,
Will MacAskill,"Likelihood that, if an existential risk in the 21st century occurs, it occurs during ""wartime or something"" [it's possible this isn't quite what he meant]",2019/2020,0.9,Yes,war,"""in terms of my estimates for existential risk over the century, I would put 90% of the risk coming from wartime or something precisely because people… If you tell me someone’s done something, a country’s done something incredibly stupid and kind of against their own interest or in some sense of global interest, it’s probably happened during a war period.""

It's not entirely clear to me what MacAskill meant. In particular, note that he did *not* explicitly say that he thinks causing there to be no war during the 21st century would reduce x-risk this century by 90% of its current level. (E.g., one could believe that a TAI transition would contribute significant x-risk whether or not a war is on, but that, given there'll likely be war at some point, that's likely to bring forward the time at which that TAI transition and dangerous effects of it occur.)",https://80000hours.org/podcast/episodes/will-macaskill-paralysis-and-hinge-of-history/#transcript,0.9
GCR Conference,Human extinction by 2100 as a result of “wars (including civil wars)”,2008,0.04,Yes,war,"This is the median. Beard et al.'s appendix says ""Note that for these predictions no time frame was given."" I think that that's incorrect, based on phrasings in the original source, but I'm not certain.",https://www.fhi.ox.ac.uk/reports/2008-1.pdf,0.04
,,,,,war,,,
Bryan Caplan,"""a world totalitarian government will emerge during the next one thousand years and last for a thousand years or more""",2006,0.05,Yes,dystopia,"""How seriously do I take the possibility that a world totalitarian government will emerge during the next one thousand years and last for a thousand years or more?  Despite the complexity and guesswork inherent in answering this question, I will hazard a response.  My unconditional probability – i.e., the probability I assign given all the information I now have - is 5%.""",The Totalitarian Threat,0.05
,,,,,dystopia,,,
Matthew Barnett,Tthe risk of human extinction due to SETI,2022,~0.1-0.2%,Yes,miscellaneous,"""I currently think there is roughly a 99% chance that one or more of the arguments I gave above imply that the risk from SETI is minimal. Absent these defeaters, I think there's perhaps a 10-20% chance that SETI will directly cause human extinction in the next 1000 years. This means I currently put the risk of human extinction due to SETI at around 0.1-0.2%. This estimate is highly non-robust.""",My current thoughts on the risks from SETI,
Holden Karnofsky,"Conditional on PASTA being developed this century, probability that we're in the most important century of all time for humanity due to there being a transition to a state in which humans as we know them are no longer the main force in world events",2021,At least 50%,Yes,miscellaneous,"""I want to roughly say that if something like PASTA is developed this century, it has at least a 50/50 chance of being the ""most important century"" in the above sense.""
Info from elsewhere on what Karnofsky means by PASTA: ""AI systems that can essentially automate all of the human activities needed to speed up scientific and technological advancement. I will call this sort of technology Process for Automating Scientific and Technological Advancement, or PASTA. (I mean PASTA to refer to either a single system or a collection of systems that can collectively do this sort of automation.)""","""Some additional detail on what I mean by ""most important century""""",
Holden Karnofsky,"Conditional on PASTA being developed this century, probability that we're in the most important century of all time for all intelligent life in our galaxy",2021,At least 25%,Yes,miscellaneous,"""I want to roughly say that if something like PASTA is developed this century, it has at least a 25% chance of being the ""most important century"" in the above sense. This is half as much as the probability for the previous version of ""most important century."" I don't mean to be precise here; I'm giving a rough indication of how likely I think such a development would be.""","""Some additional detail on what I mean by ""most important century""""",
Wei Dai,"Expected fraction of total potential value that will be lost due to people failing to deliberate correctly (e.g., failing to ever ""snap out of it"", or getting ""persuaded"" by bad memes and then asking their AIs to lock in their beliefs/values)",2021,>50%,Yes,miscellaneous,"""What's your expectation of the fraction of total potential value that will be lost due to people failing to deliberate correctly (e.g., failing to ever ""snap out of it"", or getting ""persuaded"" by bad memes and then asking their AIs to lock in their beliefs/values)? It seems to me that it's very large, easily >50%. I'm curious how others would answer this question as well.""",Comment,
Paul Christiano,Expected fraction of total potential value that will be lost due to people failing to deliberate correctly,2021,0.2,Yes,miscellaneous,"Response to Wei Dai's question shown in the above row.
""There are some fuzzy borders here, and unclarity about how to define the concept, but maybe I'd guess 10% from ""easy"" failures to deliberate (say those that could be avoided by the wisest existing humans and which might be significantly addressed, perhaps cut in half, by competitive discipline) and a further 10% from ""hard"" failures (most of which I think would not be addressed by competition).""",Comment,0.2
Toby Ord,Existential catastrophe from “other environmental damage” (not climate change) by 2120,2020,"~0.1% (~1 in 1,000)",Yes,miscellaneous,,The Precipice,
Pamlin & Armstrong,"""Infinite impact"" from an ""ecological catastrophe"" within the next 100 years, which ""refers to the state where civilization collapses and does not recover, or a situation where all human life ends"" (according to Beard et al.)",2015,0.005,No,miscellaneous,,"Pamlin, D. & Armstrong, S. (2015). Global Challenges: 12 Risks that Threaten Human Civilisation, Global Challenges Foundation.   ",0.005
Toby Ord,Existential catastrophe from “unforeseen anthropogenic risks” by 2120,2020,~3% (~1 in 30),Yes,miscellaneous,See this post for some commentary: https://forum.effectivealtruism.org/posts/Z5KZ2cui8WDjyF6gJ/my-thoughts-on-toby-ord-s-existential-risk-estimates#_Unforeseen__and__other__anthropogenic_risks__Surprisingly_risky_,The Precipice,
Toby Ord,"Existential catastrophe from “other anthropogenic risks” by 2120 (see “My thoughts on Toby Ord’s existential risk estimate” for what this means, and commentary)",2020,~2% (~1 in 50),Yes,miscellaneous,See this post for some commentary: https://forum.effectivealtruism.org/posts/Z5KZ2cui8WDjyF6gJ/my-thoughts-on-toby-ord-s-existential-risk-estimates#_Unforeseen__and__other__anthropogenic_risks__Surprisingly_risky_ ,The Precipice,
Toby Ord,“Total anthropogenic [existential] risk” by 2120,2020,~17% (~1 in 6),Yes,miscellaneous,,The Precipice,
Pamlin & Armstrong,"""Infinite impact"" from ""an uncertain risk"" within the next 100 years, which ""refers to the state where civilization collapses and does not recover, or a situation where all human life ends"" (according to Beard et al.)",2015,0.005,No,miscellaneous,"Beard et al. write: ""The following estimate concerns risks associated with scenarios that at present seem either very unlikely or very unlikely to pose a significant risk, but where there is a possibility that these assessments represent a significant underestimate of the threat. This includes physics experiments, as described in the previous section, but also risks such as animal cognitive enhancement and the search for extraterrestrial intelligent life. Pamlin and Armstrong also discuss the existential threat posed by Global System Collapse and Future Bad Global Governance, but believe that the probability of even reaching infinite threshold from such risks cannot be quantified.""","Pamlin, D. & Armstrong, S. (2015). Global Challenges: 12 Risks that Threaten Human Civilisation, Global Challenges Foundation.   ",0.005
Dave Denkenberger or ALLFED,“Reduction in far future potential due to [the risk of a?] 10% agricultural shortfall per year”,2018,0.0015,Sort-of,miscellaneous,There's some relevant info/discussion in these three sources (especially the first two): https://forum.effectivealtruism.org/posts/CcNY4MrT5QstNh4r7/cost-effectiveness-of-foods-for-global-catastrophes-even https://80000hours.org/podcast/episodes/david-denkenberger-allfed-and-feeding-everyone-no-matter-what/ https://forum.effectivealtruism.org/posts/D8t5TtSemqcGa4g7Y/my-open-for-feedback-donation-plans#1__Should_I_donate_to_ALLFED_,https://www.getguesstimate.com/models/11762,0.0015
"Anders Sandberg, adapting Denkenberger’s model",“Reduction in far future potential due to [the risk of a?] 10% agricultural shortfall per year”,2018,~0.00023%,Sort-of,miscellaneous,,https://www.getguesstimate.com/models/11691,
Brian Tomasik,"Chance that ""Humans will go extinct within millions of years for some reason other than AGI""",2015/2018,0.05,Yes,miscellaneous,"Discussed here: https://www.facebook.com/brian.tomasik/posts/10205952604908968 ""I should clarify that this probability estimate is unconditional. Most of my probability mass is on AGI taking over in the coming centuries, in which case other risks stop counting. _Conditional_ on AGI not taking over, the risk of extinction seems reasonably high, but that's partly because the main scenarios where AGI doesn't take over are scenarios where civilization collapses.""",https://reducing-suffering.org/summary-beliefs-values-big-questions/,0.05
Pablo Stafforini,"Chance that ""Humans will go extinct within millions of years for some reason other than AGI""",2015/2020,0.1,Yes,miscellaneous,,http://www.stafforini.com/blog/what_i_believe/,0.1
Brian Tomasik,"""The probability of civilizational collapse to pre-industrial levels"", but not necessarily permanent collapse (and perhaps excluding scenarios where AGI causes this?)",2015,"""Maybe ~20%?""",Yes,miscellaneous,"Tomasik was asked about his estimate of the chance that ""Humans will go extinct within millions of years for some reason other than AGI”. His reply included ""The probability of civilizational collapse to pre-industrial levels seems higher -- maybe ~20%?, but rebuilding seems reasonably likely IMHO"".",https://www.facebook.com/brian.tomasik/posts/10205952604908968,
,,,,,miscellaneous,,,
How various actions may reduce certain risks  ,,,,,miscellaneous,,,
"Ben Todd or 80,000 Hours",One plausible amount by which “$100 billion spent on reducing extinction risk” could reduce it over the next century,2017,1 percentage point,Yes,miscellaneous,I’m not sure where this “1 percentage point reduction” lies on the continuum from “genuine guess” to “number used purely for the sake of illustration”. ,https://80000hours.org/articles/extinction-risk/,
Paul Christiano,Amount by which “really nailing” some portion of AI safety work could improve the expected value of the future,2019,0.05,Yes,miscellaneous,"He says ""All of the numbers I’m going to give are very made up though. If you asked me a second time you’ll get all different numbers.""",https://aiimpacts.org/conversation-with-paul-christiano/,0.05
Paul Christiano,Amount by which “a marginal person” doing some portion of AI safety work could “easily” improve the expected value of the future,2019,"0.005% (“one in 20,000 or something”)",Yes,miscellaneous,"He says ""All of the numbers I’m going to give are very made up though. If you asked me a second time you’ll get all different numbers.""",https://aiimpacts.org/conversation-with-paul-christiano/,
