estimator,estimation_measure,date,estimation,source_read_by_estimator,risk_category,other_notes,source,estimation_numeric
Buck Shlegris,"""Overall P(doom)"" (from context, this seems to be a prediction just about P(doom) from AI, or seems to be expecting almost 100% of P(doom) is accounted for by AI, but that's not specified)",2023,0.25,Yes,ai,"""My overall P(doom) is like 25% (though it fluctuates wildly""",https://youtu.be/YTlrPeikoyw?t=1788,0.25
Bensinger's survey of 44 people working on long-term AI risk,"""How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of humanity not doing enough technical AI safety research?""",2021,"Mean: ~30%
Median: ~20%",Yes,ai,"""I sent a two-question survey to ~117 people working on long-term AI risk [...] 44 people responded (~38% response rate). In all cases, these represent the views of specific individuals, not an official view of any organization. Since some people's views may have made them more/less likely to respond, I suggest caution in drawing strong conclusions from the results below. Another reason for caution is that respondents added a lot of caveats to their responses (see the anonymized spreadsheet)"".
Bensinger sent the survey directly to ""MIRI's research team, and people who recently left OpenAI (mostly people suggested by Beth Barnes of OpenAI). I [also] sent it to five other groups through org representatives (who I asked to send it to everyone at the org ""who researches long-term AI topics, or who has done a lot of past work on such topics""): OpenAI, the Future of Humanity Institute (FHI), DeepMind, the Center for Human-Compatible AI (CHAI), and Open Philanthropy""
Further details and caveats in the post.",https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results,
Bensinger's survey of 44 people working on long-term AI risk,"""How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of AI systems not doing/optimizing what the people deploying them wanted/intended?""",2021,"Mean: ~40%
Median: ~30%",Yes,ai,,https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results,
Joe Carlsmith,"Existential catastrophe by 2070 from scenarios involving ""power-seeking AI"" and some specific conditions being met",2021,~5%,No,ai,"""My current, highly-unstable, subjective estimate is that there is a ~5% percent chance of existential catastrophe by 2070 from scenarios in which (1)-(6) are true"".
This comes from a draft report. By the time you're reading this, Carlsmith may have updated his estimate without me updating what's shown here.",https://forum.effectivealtruism.org/posts/78NoGoRitPzeT8nga/draft-report-on-existential-risk-from-power-seeking-ai,
Survey respondents,Probability of “doom” before 2070 due to the type of problem discussed in Carlsmith's report on existential risk from power-seeking AI,2021,Survey answers ranged from <1% to >50%,No,ai,"My only knowledge of this survey comes from a footnote in Karnofsky's ""Most Important Century"" series which says: ""A more detailed, private survey done for this report, asking about the probability of “doom” before 2070 due to the type of problem discussed in the report, got answers ranging from <1% to >50%. In my opinion, there are very thoughtful people who have seriously considered these matters at both ends of that range.""
2021 is just my guess as to when the survey would've been sent out.",,
Beth Barnes,"""the [AI] persuasion problem as distinct from the alignment problem""",2021,10x smaller than the alignment problem,Yes,ai,"""Overall I think this threat is significantly smaller than more standard alignment failure scenarios (maybe 10x smaller), but comparable enough that interventions could be well worthwhile if they're fairly tractable. [...] How to divide the space is a bit confusing here; I’d say something like ‘the persuasion problem as distinct from the alignment problem’ is 10x smaller, but in fact there’s some overlap, so it might also be reasonable to say something like ‘¼ of alignment-failure-esque xrisk scenarios will have a significant societal-scale persuasion component’, and almost all will have some deception component (and the fact that it’s hard to train your AI to be honest with you will be a key problem)"". See the source for more on what Barnes means by ""the persuasion problem"". ",https://www.alignmentforum.org/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion,
Rohin Shah,"Probability of existential catastrophe due to the risks highlighted by the ecology / GPT, and ARCHES perspectives",2020,Similar to 10%,Yes,ai,"Rohin summarises the ecology / GPT perspectives from Dafoe (2020) and the ARCHES paper, then writes ""I’ve estimated (AN #80) a 10% chance of existential catastrophe via a failure of intent alignment, absent intervention from longtermists to address intent alignment. Estimates vary quite a lot, even among people who have thought about the problem a lot; I’ve heard as low as < 1% and as high as 80% (though these usually don’t assume “no intervention from longtermists”). It’s harder to estimate the importance of structural risks and extinction risks highlighted in the two summaries above, but the arguments in the previous two posts seem reasonably compelling and I think I’d be inclined to assign a similar importance to it (i.e. similar probability of causing an existential catastrophe). Note that this means I’m disagreeing with Critch: he believes that we are far more likely to go extinct through effects unique to multi-multi dynamics; in contrast I find the argument less persuasive because we do have governance, regulations, national security etc. that would already be trying to mitigate issues that arise in multi-multi contexts, especially things that could plausibly cause extinction.""
This wasn't exactly a quantitative estimate, so is arguably out of scope for this database, but seemed worth including.",https://www.alignmentforum.org/posts/8eX8DJctsACtR2sfX/an-118-risks-solutions-and-prioritization-in-a-world-with,
Toby Ord,Existential catastrophe by 2120 as a result of “unaligned AI”,2020,~10%,Yes,ai,Commentary here: https://forum.effectivealtruism.org/posts/Z5KZ2cui8WDjyF6gJ/some-thoughts-on-toby-ord-s-existential-risk-estimates#What_types_of_catastrophe_are_included_in_the__Unaligned_AI__estimate_,The Precipice,
Rob Wiblin,"""the risk of a severe, even existential catastrophe caused by machine intelligence within the next 100 years""",2017,"""something like 10%""",Yes,ai,"It's unclear what's being estimated here, since ""severe catastrophe"" and ""existential catastrophe"" are extremely different bars. 
I think there's also no indication of how this estimate was arrived at. But this comment is made regarding the article as a whole: ""This profile is based on interviews with: Professor Nick Bostrom at the University of Oxford, the author of Superintelligence; an anonymous leading professor of computer science; Jaan Tallinn, one of the largest donors in the space and the co-founder of Skype; Jan Leike, a machine learning researcher now at DeepMind; Miles Brundage, an AI policy researcher at the Future of Humanity Institute at Oxford University; Nate Soares, the Executive Director of the Machine Intelligence Research Institute; Daniel Dewey, who works full-time finding researchers and funding opportunities in the field; and several other researchers in the area. We also read advice from David Krueger, a Machine Learning PhD student.""","""Positively shaping the development of artificial intelligence""",
Global Catastrophic Risk Conference,Human extinction by 2100 as a result of “superintelligent AI”,2008,0.05,Yes,ai,"This is the median. Beard et al.'s appendix says ""Note that for these predictions no time frame was given."" I think that that's incorrect, based on phrasings in the original source, but I'm not certain.",https://www.fhi.ox.ac.uk/reports/2008-1.pdf,0.05
Survey of “AI experts”,“Extremely bad (e.g. extinction)” long-run impact on humanity from “high-level machine intelligence”,2017,0.05,Yes,ai,"The report's authors discuss potential concerns around non-response bias and the fact that “NIPS and ICML authors are representative of machine learning but not of the field of artificial intelligence as a whole”. There was also evidence of apparent inconsistencies in estimates of AI timelines as a result of small changes to how questions were asked, providing further reason to wonder how meaningful these experts’ predictions were. https://web.archive.org/web/20171030220008/https://aiimpacts.org/some-survey-results/",https://arxiv.org/abs/1705.08807,0.05
Pamlin & Armstrong,"""Infinite impact"" from AI within the next 100 years, which ""refers to the state where civilization collapses and does not recover, or a situation where all human life ends"" (according to Beard et al.)",2015,0-10%,No,ai,,"Pamlin, D. & Armstrong, S. (2015). Global Challenges: 12 Risks that Threaten Human Civilisation, Global Challenges Foundation.   ",
Ben Garfinkel,"""AI causing an existential catastrophe in the next century""",2020,~0.1-1%,Yes,ai,"Garfinkel was asked for his estimate during an AMA, and replied ""I currently give it something in the .1%-1% range.""",https://forum.effectivealtruism.org/posts/7gxtXrMeqw78ZZeY9/ama-or-discuss-my-80k-podcast-episode-ben-garfinkel-fhi?commentId=uxiKooRc6d7JpjMSg,
Rohin Shah,"Chance that AI, through “adversarial optimization against humans only”, will cause existential catastrophe",2020,~5%,Yes,ai,"This is my interpretation of some comments that may not have been meant to be taken very literally. Elsewhere, Rohin noted that this was “[his] opinion before updating on other people's views"": https://forum.effectivealtruism.org/posts/tugs9KQyNqi4yRTsb/does-80-000-hours-focus-too-much-on-ai-risk#ZmtPji3pQaZK7Y4FF I think he updated this in 2020 to ~9%, due to pessimism about discontinuous scenarios: https://www.lesswrong.com/posts/TdwpN484eTbPSvZkm/rohin-shah-on-reasons-for-ai-optimism?commentId=n577gwGB3vRpwkBmj Rohin also discusses his estimates here: https://futureoflife.org/2020/04/15/an-overview-of-technical-ai-alignment-in-2018-and-2019-with-buck-shlegeris-and-rohin-shah/
In 2021, Rohin similarly said ""Usually when I’m asked to predict this, the actual prediction I give is, probability that we go extinct due to an intent alignment failure, and then depending on the situation I will either condition on… I will either make that unconditional, so that includes all of the things that people will do to try to prevent that from happening. Or, I make it conditional, on the long-termist community doesn’t do anything, or vanishes or something. But even in that world, there’s still… Everyone who’s not a long-termist, who can still prevent that from happening, which I really do expect them to do, and then I think I give my cached answer, on both of those is like 5% and 10% respectively, which I think is probably the numbers I gave you. If I actually sat down and try to like come up with a probability, I would probably come up with something different this time, but I have not done that, and I’m way too anchored on those previous estimates, to be able to give you a new estimate this time.""",https://www.lesswrong.com/posts/TdwpN484eTbPSvZkm/rohin-shah-on-reasons-for-ai-optimism,
Buck Shlegris,"""the probability of AI-induced existential risk"" (but from context, I believe this actually meant the probability of an AI-induced existential catastrophe)",2020,0.5,Yes,ai,"Note that Buck gave a 25 percentage point lower estimate for seemingly the same question in 2023, as also captured in this spreadsheet. ",https://futureoflife.org/2020/04/15/an-overview-of-technical-ai-alignment-in-2018-and-2019-with-buck-shlegeris-and-rohin-shah/,0.5
James Fodor,Existential risk from unaligned AI over the coming 100 years,2020,0.0005,Yes,ai,"This was a direct response to Ord's estimate. It focuses on one pathway to x-risk from AI, not all pathways (e.g., not AI misuse or risks from competition between powerful AIs). ""These estimates should not be taken very seriously. I do not believe we have enough information to make sensible quantitative estimates about these eventualities. Nevertheless, I present my estimates largely in order to illustrate the extent of my disagreement with Ord’s estimates, and to illustrate the key considerations I examine in order to arrive at an estimate."" In comments on the source, Rohin Shah critiques some of the inputs to this estimate, and provides his own, substantially higher estimates.",Critical Review of 'The Precipice': A Reassessment of the Risks of AI and Pandemics,0.0005
Stuart Armstrong,Existential risk from AI,2020,5-30%,Yes,ai,"""I put the probability that [AI/AGI] is an existential risk roughly in the 30% to 5% range, depending on how the problem is phrased."" I assume he means the probability of existential catastrophe from AI/AGI, not the probability that AI/AGI poses an existential risk. ",https://youtu.be/WLXuZtWoRcE?t=1229,
Stuart Armstrong,Chance of humanity not surviving AI,2014,"50, 40, or 33%",Yes,ai,"Stated verbally during an interview. Not totally clear precisely what was being estimated (e.g. just extinction, or existential catastrophe more broadly?). He noted ""This number fluctuates a lot"". He indicated he thought we had a 2/3 chance of surviving, then said he'd adjust to 50%, which is his number for an ""actually superintelligent"" AI, whereas for ""AI in general"" it'd be 60%. This is notably higher than his 2020 estimate, implying either that he updated towards somewhat more ""optimism"" between 2014 and 2020, or that one or both of these estimates don't reflect stable beliefs.",https://www.youtube.com/watch?v=i4LjoJGpqIY& (from 39:40),
Paul Christiano,Amount by which risk of failure to align AI (using only a narrow conception of alignment) reduces the expected value of the future,2019,~10%,Yes,ai,"He also says ""I made up 10%, it’s kind of a random number."" And ""All of the numbers I’m going to give are very made up though. If you asked me a second time you’ll get all different numbers.""

Christiano gave a similar estimate in a 2021 podcast interview: 
""Daniel Filan: [...] the first topic I want to talk about is this idea that AI might pose some kind of existential threat or an existential risk, and there’s this common definition of existential risk, which is a risk of something happening that would incapacitate humanity and limit its possibilities for development, incredibly drastically in a way comparable to human extinction, such as human extinction. Is that roughly the definition you use?

Paul Christiano: Yeah. I think I don’t necessarily have a bright line around giant or drastic drops versus moderate drops. I often think in terms of the expected fraction of humanity’s potential that is lost. But yeah, that’s basically what I think of it. Anything that could cause us not to fulfill some large chunk of our potential. I think of AI in particular, a failure to align AI maybe makes the future, in my guess 10% or 20% worse, or something like that, in expectation. And that makes it one of the worst things. I mean, not the worst, that’s a minority of all of our failure to fall short of our potential, but it’s a lot of failure to fall short of our potential. You can’t have that many 20% hits before you’re down to no potential left.

Daniel Filan: Yeah. When you say a 10% or 20% hit to human potential in expectation, do you mean if we definitely failed to align AI or do you mean we may or may not fail to align AI and overall that uncertainty equates to a 20%, or 10% to 20% hit?

Paul Christiano: Yeah, that’s unconditionally. So I think if you told me we definitely mess up alignment maximally then I’m more like, oh, now I are looking at a pretty big, close to 100% drop. I wouldn’t go all the way to 100.""",https://aiimpacts.org/conversation-with-paul-christiano/,
Jaan Tallinn,"""likelihood of an existential catastrophe happening this century"" (maybe just from AI?)",2020,33-50%,Yes,ai,"This comes from a verbal interview (from the 14:14 mark). The interview was focused on AI, and this estimate may have been as well. Tallinn said he's not very confident, but is fairly confident his estimate would be in double-digits, and then said ""two obvious Schelling points"" are 33% or 50%, so he'd guess somewhere in between those. Other comments during the interview seem to imply Tallinn is either just talking about extinction risk or thinks existential risk happens to be dominated by extinction risk.",https://youtu.be/aFAI8itZCGk?t=854,
,,,,,ai,,,
