estimator,estimation_measure,date,estimation,source_read_by_estimator,risk_category,other_notes,source,estimation_numeric
Matthew Barnett,Tthe risk of human extinction due to SETI,2022,~0.1-0.2%,Yes,miscellaneous,"""I currently think there is roughly a 99% chance that one or more of the arguments I gave above imply that the risk from SETI is minimal. Absent these defeaters, I think there's perhaps a 10-20% chance that SETI will directly cause human extinction in the next 1000 years. This means I currently put the risk of human extinction due to SETI at around 0.1-0.2%. This estimate is highly non-robust.""",My current thoughts on the risks from SETI,
Holden Karnofsky,"Conditional on PASTA being developed this century, probability that we're in the most important century of all time for humanity due to there being a transition to a state in which humans as we know them are no longer the main force in world events",2021,At least 50%,Yes,miscellaneous,"""I want to roughly say that if something like PASTA is developed this century, it has at least a 50/50 chance of being the ""most important century"" in the above sense.""
Info from elsewhere on what Karnofsky means by PASTA: ""AI systems that can essentially automate all of the human activities needed to speed up scientific and technological advancement. I will call this sort of technology Process for Automating Scientific and Technological Advancement, or PASTA. (I mean PASTA to refer to either a single system or a collection of systems that can collectively do this sort of automation.)""","""Some additional detail on what I mean by ""most important century""""",
Holden Karnofsky,"Conditional on PASTA being developed this century, probability that we're in the most important century of all time for all intelligent life in our galaxy",2021,At least 25%,Yes,miscellaneous,"""I want to roughly say that if something like PASTA is developed this century, it has at least a 25% chance of being the ""most important century"" in the above sense. This is half as much as the probability for the previous version of ""most important century."" I don't mean to be precise here; I'm giving a rough indication of how likely I think such a development would be.""","""Some additional detail on what I mean by ""most important century""""",
Wei Dai,"Expected fraction of total potential value that will be lost due to people failing to deliberate correctly (e.g., failing to ever ""snap out of it"", or getting ""persuaded"" by bad memes and then asking their AIs to lock in their beliefs/values)",2021,>50%,Yes,miscellaneous,"""What's your expectation of the fraction of total potential value that will be lost due to people failing to deliberate correctly (e.g., failing to ever ""snap out of it"", or getting ""persuaded"" by bad memes and then asking their AIs to lock in their beliefs/values)? It seems to me that it's very large, easily >50%. I'm curious how others would answer this question as well.""",Comment,
Paul Christiano,Expected fraction of total potential value that will be lost due to people failing to deliberate correctly,2021,0.2,Yes,miscellaneous,"Response to Wei Dai's question shown in the above row.
""There are some fuzzy borders here, and unclarity about how to define the concept, but maybe I'd guess 10% from ""easy"" failures to deliberate (say those that could be avoided by the wisest existing humans and which might be significantly addressed, perhaps cut in half, by competitive discipline) and a further 10% from ""hard"" failures (most of which I think would not be addressed by competition).""",Comment,0.2
Toby Ord,Existential catastrophe from “other environmental damage” (not climate change) by 2120,2020,"~0.1% (~1 in 1,000)",Yes,miscellaneous,,The Precipice,
Pamlin & Armstrong,"""Infinite impact"" from an ""ecological catastrophe"" within the next 100 years, which ""refers to the state where civilization collapses and does not recover, or a situation where all human life ends"" (according to Beard et al.)",2015,0.005,No,miscellaneous,,"Pamlin, D. & Armstrong, S. (2015). Global Challenges: 12 Risks that Threaten Human Civilisation, Global Challenges Foundation.   ",0.005
Toby Ord,Existential catastrophe from “unforeseen anthropogenic risks” by 2120,2020,~3% (~1 in 30),Yes,miscellaneous,See this post for some commentary: https://forum.effectivealtruism.org/posts/Z5KZ2cui8WDjyF6gJ/my-thoughts-on-toby-ord-s-existential-risk-estimates#_Unforeseen__and__other__anthropogenic_risks__Surprisingly_risky_,The Precipice,
Toby Ord,"Existential catastrophe from “other anthropogenic risks” by 2120 (see “My thoughts on Toby Ord’s existential risk estimate” for what this means, and commentary)",2020,~2% (~1 in 50),Yes,miscellaneous,See this post for some commentary: https://forum.effectivealtruism.org/posts/Z5KZ2cui8WDjyF6gJ/my-thoughts-on-toby-ord-s-existential-risk-estimates#_Unforeseen__and__other__anthropogenic_risks__Surprisingly_risky_ ,The Precipice,
Toby Ord,“Total anthropogenic [existential] risk” by 2120,2020,~17% (~1 in 6),Yes,miscellaneous,,The Precipice,
Pamlin & Armstrong,"""Infinite impact"" from ""an uncertain risk"" within the next 100 years, which ""refers to the state where civilization collapses and does not recover, or a situation where all human life ends"" (according to Beard et al.)",2015,0.005,No,miscellaneous,"Beard et al. write: ""The following estimate concerns risks associated with scenarios that at present seem either very unlikely or very unlikely to pose a significant risk, but where there is a possibility that these assessments represent a significant underestimate of the threat. This includes physics experiments, as described in the previous section, but also risks such as animal cognitive enhancement and the search for extraterrestrial intelligent life. Pamlin and Armstrong also discuss the existential threat posed by Global System Collapse and Future Bad Global Governance, but believe that the probability of even reaching infinite threshold from such risks cannot be quantified.""","Pamlin, D. & Armstrong, S. (2015). Global Challenges: 12 Risks that Threaten Human Civilisation, Global Challenges Foundation.   ",0.005
Dave Denkenberger or ALLFED,“Reduction in far future potential due to [the risk of a?] 10% agricultural shortfall per year”,2018,0.0015,Sort-of,miscellaneous,There's some relevant info/discussion in these three sources (especially the first two): https://forum.effectivealtruism.org/posts/CcNY4MrT5QstNh4r7/cost-effectiveness-of-foods-for-global-catastrophes-even https://80000hours.org/podcast/episodes/david-denkenberger-allfed-and-feeding-everyone-no-matter-what/ https://forum.effectivealtruism.org/posts/D8t5TtSemqcGa4g7Y/my-open-for-feedback-donation-plans#1__Should_I_donate_to_ALLFED_,https://www.getguesstimate.com/models/11762,0.0015
"Anders Sandberg, adapting Denkenberger’s model",“Reduction in far future potential due to [the risk of a?] 10% agricultural shortfall per year”,2018,~0.00023%,Sort-of,miscellaneous,,https://www.getguesstimate.com/models/11691,
Brian Tomasik,"Chance that ""Humans will go extinct within millions of years for some reason other than AGI""",2015/2018,0.05,Yes,miscellaneous,"Discussed here: https://www.facebook.com/brian.tomasik/posts/10205952604908968 ""I should clarify that this probability estimate is unconditional. Most of my probability mass is on AGI taking over in the coming centuries, in which case other risks stop counting. _Conditional_ on AGI not taking over, the risk of extinction seems reasonably high, but that's partly because the main scenarios where AGI doesn't take over are scenarios where civilization collapses.""",https://reducing-suffering.org/summary-beliefs-values-big-questions/,0.05
Pablo Stafforini,"Chance that ""Humans will go extinct within millions of years for some reason other than AGI""",2015/2020,0.1,Yes,miscellaneous,,http://www.stafforini.com/blog/what_i_believe/,0.1
Brian Tomasik,"""The probability of civilizational collapse to pre-industrial levels"", but not necessarily permanent collapse (and perhaps excluding scenarios where AGI causes this?)",2015,"""Maybe ~20%?""",Yes,miscellaneous,"Tomasik was asked about his estimate of the chance that ""Humans will go extinct within millions of years for some reason other than AGI”. His reply included ""The probability of civilizational collapse to pre-industrial levels seems higher -- maybe ~20%?, but rebuilding seems reasonably likely IMHO"".",https://www.facebook.com/brian.tomasik/posts/10205952604908968,
,,,,,miscellaneous,,,
How various actions may reduce certain risks  ,,,,,miscellaneous,,,
"Ben Todd or 80,000 Hours",One plausible amount by which “$100 billion spent on reducing extinction risk” could reduce it over the next century,2017,1 percentage point,Yes,miscellaneous,I’m not sure where this “1 percentage point reduction” lies on the continuum from “genuine guess” to “number used purely for the sake of illustration”. ,https://80000hours.org/articles/extinction-risk/,
Paul Christiano,Amount by which “really nailing” some portion of AI safety work could improve the expected value of the future,2019,0.05,Yes,miscellaneous,"He says ""All of the numbers I’m going to give are very made up though. If you asked me a second time you’ll get all different numbers.""",https://aiimpacts.org/conversation-with-paul-christiano/,0.05
Paul Christiano,Amount by which “a marginal person” doing some portion of AI safety work could “easily” improve the expected value of the future,2019,"0.005% (“one in 20,000 or something”)",Yes,miscellaneous,"He says ""All of the numbers I’m going to give are very made up though. If you asked me a second time you’ll get all different numbers.""",https://aiimpacts.org/conversation-with-paul-christiano/,
